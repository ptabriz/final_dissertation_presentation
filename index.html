<!doctype html>
<html lang="en">

    <head>
        <meta charset="utf-8">

        <title>Dissertation presentation</title>

        <meta name="description" content="Tangible Landscape slides">
        <meta name="author" content="NCSU GeoForAll Lab members">

        <meta name="apple-mobile-web-app-capable" content="yes" />
        <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />

        <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

        <link rel="stylesheet" href="css/reveal.css">
        <link rel="stylesheet" href="css/theme/payam_grey.css" id="theme">
        <link rel="stylesheet" href="./css/style.css">

        <!-- For syntax highlighting -->
        <link rel="stylesheet" href="lib/css/zenburn.css">

        <!-- For chalkboard plugin -->
        <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.5.0/css/font-awesome.min.css">


    </style><style type="text/css">.MJXp-script {font-size: .8em}
      .MJXp-right {-webkit-transform-origin: right; -moz-transform-origin: right; -ms-transform-origin: right; -o-transform-origin: right; transform-origin: right}
      .MJXp-bold {font-weight: bold}
      .MJXp-italic {font-style: italic}
      .MJXp-scr {font-family: MathJax_Script,'Times New Roman',Times,STIXGeneral,serif}
      .MJXp-frak {font-family: MathJax_Fraktur,'Times New Roman',Times,STIXGeneral,serif}
      .MJXp-sf {font-family: MathJax_SansSerif,'Times New Roman',Times,STIXGeneral,serif}
      .MJXp-cal {font-family: MathJax_Caligraphic,'Times New Roman',Times,STIXGeneral,serif}
      .MJXp-mono {font-family: MathJax_Typewriter,'Times New Roman',Times,STIXGeneral,serif}
      .MJXp-largeop {font-size: 150%}
      .MJXp-largeop.MJXp-int {vertical-align: -.2em}
      .MJXp-math {display: inline-block; line-height: 1.2; text-indent: 0; font-family: 'Times New Roman',Times,STIXGeneral,serif; white-space: nowrap; border-collapse: collapse}
      .MJXp-display {display: block; text-align: center; margin: 1em 0}
      .MJXp-math span {display: inline-block}
      .MJXp-box {display: block!important; text-align: center}
      .MJXp-box:after {content: " "}
      .MJXp-rule {display: block!important; margin-top: .1em}
      .MJXp-char {display: block!important}
      .MJXp-mo {margin: 0 .15em}
      .MJXp-mfrac {margin: 0 .125em; vertical-align: .25em}
      .MJXp-denom {display: inline-table!important; width: 100%}
      .MJXp-denom > * {display: table-row!important}
      .MJXp-surd {vertical-align: top}
      .MJXp-surd > * {display: block!important}
      .MJXp-script-box > *  {display: table!important; height: 50%}
      .MJXp-script-box > * > * {display: table-cell!important; vertical-align: top}
      .MJXp-script-box > *:last-child > * {vertical-align: bottom}
      .MJXp-script-box > * > * > * {display: block!important}
      .MJXp-mphantom {visibility: hidden}
      .MJXp-munderover {display: inline-table!important}
      .MJXp-over {display: inline-block!important; text-align: center}
      .MJXp-over > * {display: block!important}
      .MJXp-munderover > * {display: table-row!important}
      .MJXp-mtable {vertical-align: .25em; margin: 0 .125em}
      .MJXp-mtable > * {display: inline-table!important; vertical-align: middle}
      .MJXp-mtr {display: table-row!important}
      .MJXp-mtd {display: table-cell!important; text-align: center; padding: .5em 0 0 .5em}
      .MJXp-mtr > .MJXp-mtd:first-child {padding-left: 0}
      .MJXp-mtr:first-child > .MJXp-mtd {padding-top: 0}
      .MJXp-mlabeledtr {display: table-row!important}
      .MJXp-mlabeledtr > .MJXp-mtd:first-child {padding-left: 0}
      .MJXp-mlabeledtr:first-child > .MJXp-mtd {padding-top: 0}
      .MJXp-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 1px 3px; font-style: normal; font-size: 90%}
      .MJXp-scale0 {-webkit-transform: scaleX(.0); -moz-transform: scaleX(.0); -ms-transform: scaleX(.0); -o-transform: scaleX(.0); transform: scaleX(.0)}
      .MJXp-scale1 {-webkit-transform: scaleX(.1); -moz-transform: scaleX(.1); -ms-transform: scaleX(.1); -o-transform: scaleX(.1); transform: scaleX(.1)}
      .MJXp-scale2 {-webkit-transform: scaleX(.2); -moz-transform: scaleX(.2); -ms-transform: scaleX(.2); -o-transform: scaleX(.2); transform: scaleX(.2)}
      .MJXp-scale3 {-webkit-transform: scaleX(.3); -moz-transform: scaleX(.3); -ms-transform: scaleX(.3); -o-transform: scaleX(.3); transform: scaleX(.3)}
      .MJXp-scale4 {-webkit-transform: scaleX(.4); -moz-transform: scaleX(.4); -ms-transform: scaleX(.4); -o-transform: scaleX(.4); transform: scaleX(.4)}
      .MJXp-scale5 {-webkit-transform: scaleX(.5); -moz-transform: scaleX(.5); -ms-transform: scaleX(.5); -o-transform: scaleX(.5); transform: scaleX(.5)}
      .MJXp-scale6 {-webkit-transform: scaleX(.6); -moz-transform: scaleX(.6); -ms-transform: scaleX(.6); -o-transform: scaleX(.6); transform: scaleX(.6)}
      .MJXp-scale7 {-webkit-transform: scaleX(.7); -moz-transform: scaleX(.7); -ms-transform: scaleX(.7); -o-transform: scaleX(.7); transform: scaleX(.7)}
      .MJXp-scale8 {-webkit-transform: scaleX(.8); -moz-transform: scaleX(.8); -ms-transform: scaleX(.8); -o-transform: scaleX(.8); transform: scaleX(.8)}
      .MJXp-scale9 {-webkit-transform: scaleX(.9); -moz-transform: scaleX(.9); -ms-transform: scaleX(.9); -o-transform: scaleX(.9); transform: scaleX(.9)}
      .MathJax_PHTML .noError {vertical-align: ; font-size: 90%; text-align: left; color: black; padding: 1px 3px; border: 1px solid}
      </style><style type="text/css">.MathJax_Display {text-align: center; margin: 1em 0em; position: relative; display: block!important; text-indent: 0; max-width: none; max-height: none; min-width: 0; min-height: 0; width: 100%}
      .MathJax .merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 1px 3px; font-style: normal; font-size: 90%}
      .MathJax .MJX-monospace {font-family: monospace}
      .MathJax .MJX-sans-serif {font-family: sans-serif}
      #MathJax_Tooltip {background-color: InfoBackground; color: InfoText; border: 1px solid black; box-shadow: 2px 2px 5px #AAAAAA; -webkit-box-shadow: 2px 2px 5px #AAAAAA; -moz-box-shadow: 2px 2px 5px #AAAAAA; -khtml-box-shadow: 2px 2px 5px #AAAAAA; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true'); padding: 3px 4px; z-index: 401; position: absolute; left: 0; top: 0; width: auto; height: auto; display: none}
      .MathJax {display: inline; font-style: normal; font-weight: normal; line-height: normal; font-size: 100%; font-size-adjust: none; text-indent: 0; text-align: left; text-transform: none; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; padding: 0; margin: 0}
      .MathJax:focus, body :focus .MathJax {display: inline-table}
      .MathJax.MathJax_FullWidth {text-align: center; display: table-cell!important; width: 10000em!important}
      .MathJax img, .MathJax nobr, .MathJax a {border: 0; padding: 0; margin: 0; max-width: none; max-height: none; min-width: 0; min-height: 0; vertical-align: 0; line-height: normal; text-decoration: none}
      img.MathJax_strut {border: 0!important; padding: 0!important; margin: 0!important; vertical-align: 0!important}
      .MathJax span {display: inline; position: static; border: 0; padding: 0; margin: 0; vertical-align: 0; line-height: normal; text-decoration: none}
      .MathJax nobr {white-space: nowrap!important}
      .MathJax img {display: inline!important; float: none!important}
      .MathJax * {transition: none; -webkit-transition: none; -moz-transition: none; -ms-transition: none; -o-transition: none}
      .MathJax_Processing {visibility: hidden; position: fixed; width: 0; height: 0; overflow: hidden}
      .MathJax_Processed {display: none!important}
      .MathJax_ExBox {display: block!important; overflow: hidden; width: 1px; height: 60ex; min-height: 0; max-height: none}
      .MathJax .MathJax_EmBox {display: block!important; overflow: hidden; width: 1px; height: 60em; min-height: 0; max-height: none}
      .MathJax_LineBox {display: table!important}
      .MathJax_LineBox span {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}
      .MathJax .MathJax_HitBox {cursor: text; background: white; opacity: 0; filter: alpha(opacity=0)}
      .MathJax .MathJax_HitBox * {filter: none; opacity: 1; background: transparent}
      #MathJax_Tooltip * {filter: none; opacity: 1; background: transparent}
      @font-face {font-family: MathJax_Main; src: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff?V=2.7.0') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf?V=2.7.0') format('opentype')}
      @font-face {font-family: MathJax_Main-bold; src: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff?V=2.7.0') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf?V=2.7.0') format('opentype')}
      @font-face {font-family: MathJax_Main-italic; src: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff?V=2.7.0') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf?V=2.7.0') format('opentype')}
      @font-face {font-family: MathJax_Math-italic; src: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff?V=2.7.0') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf?V=2.7.0') format('opentype')}
      @font-face {font-family: MathJax_Caligraphic; src: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff?V=2.7.0') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf?V=2.7.0') format('opentype')}
      @font-face {font-family: MathJax_Size1; src: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff?V=2.7.0') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf?V=2.7.0') format('opentype')}
      @font-face {font-family: MathJax_Size2; src: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff?V=2.7.0') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf?V=2.7.0') format('opentype')}
      @font-face {font-family: MathJax_Size3; src: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff?V=2.7.0') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf?V=2.7.0') format('opentype')}
      @font-face {font-family: MathJax_Size4; src: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff?V=2.7.0') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf?V=2.7.0') format('opentype')}
      .MathJax .noError {vertical-align: ; font-size: 90%; text-align: left; color: black; padding: 1px 3px; border: 1px solid}
      </style><style type="text/css">@font-face {font-family: MathJax_AMS; src: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff?V=2.7.0') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf?V=2.7.0') format('opentype')}
      </style></head><body><div style="visibility: hidden; overflow: hidden; position: absolute; top: 0px; height: 1px; width: auto; padding: 0px; border: 0px; margin: 0px; text-align: left; text-indent: 0px; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal;">
        <div id="MathJax_Hidden"><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br></div></div><div id="MathJax_Message" style="display: none;"></div>.

        <header style="position: absolute;top: 50px; z-index:500; font-size:100px;background-color: rgba(0,0,0,0.5)"></header>

        <style type="text/css">

          html.dimbg_095 .slide-background.present {
            opacity: 0.95 !important;
          }
          html.dimbg_08 .slide-background.present {
            opacity: 0.8 !important;
          }
          html.dimbg_06 .slide-background.present {
            opacity: 0.6 !important;
          }
          html.dimbg_05 .slide-background.present {
            opacity: 0.5 !important;
          }
          html.dimbg_04 .slide-background.present {
            opacity: 0.4 !important;
          }
          html.dimbg_03 .slide-background.present {
            opacity: 0.2 !important;
          }
          html.dimbg_02 .slide-background.present {
            opacity: 0.2 !important;
          }
          html.dimbg_01 .slide-background.present{
            opacity: 0.1 !important;
          }
          .overl {
            background-color:rgba(0,0,0,0.9);
          }
          .overlaybottom{
            background-color:rgba(0,0,0,0.9);
            margin-bottom: 20% !important;
          }
          .overlaytop{
            background-color:rgba(0,0,0,0.9);
            margin-top: 20% !important;
          }
          .overlayleft{
            background-color:rgba(0,0,0,0.9);
            margin-right: 50% !important;
          }
          .overlayright{
            background-color:rgba(0,0,0,0.9);
            margin-left: 50% !important;
          }

          .transp_bottom {
              position:absolute;
              left:0;
              top: 350px;
              background: rgba(0,0,0,.8);
              height:1000px;
              width: 100%;
          }
          .transp_top {
              position:absolute;
              left:0;
              top: -600px;
              background: rgba(0,0,0,.8);
              height:600px;
              width: 100%;
          }

        </style>

        <!-- Printing and PDF exports -->
        <script>
          var link = document.createElement( 'link' );
          link.rel = 'stylesheet';
          link.type = 'text/css';
          link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
          document.getElementsByTagName( 'head' )[0].appendChild( link );
        </script>



        <!-- If the query includes 'print-pdf', include the PDF print sheet -->
        <script>
            if( window.location.search.match( /print-pdf/gi ) ) {
                var link = document.createElement( 'link' );
                link.rel = 'stylesheet';
                link.type = 'text/css';
                link.href = 'css/print/pdf.css';
                document.getElementsByTagName( 'head' )[0].appendChild( link );
            }
        </script>

        <!--[if lt IE 9]>
        <script src="lib/js/html5shiv.js"></script>
        <![endif]-->

        <style>
        body {
        background-color: #FFF !important;*/
        /*
          background-image: url("pictures/elevation-nagshead.gif");
          background-repeat: no-repeat;
          background-position: left bottom;
        }
        .reveal section img {
            background: transparent;
            border: 0;
            box-shadow: 0 0 0 rgba(0, 0, 0, 0.15);
        }
        /* for standalone frame */
        /*
        iframe {
            display: block;
            margin-left: auto;
            margin-right: auto;
        }
        */
        /* display: inline; background-color: #002B36; padding: 0px; margin: 0px */
        .rounded-corners {
            border: 0px solid black;
            border-radius: 5px;
            -moz-border-radius: 5px;
            -khtml-border-radius: 5px;
            -webkit-border-radius: 5px;
        }
        a:hover {
            color: #444 !important;
            text-decoration: underline !important;
        }
        h1, h2, h3, h4, h5 {
            text-transform: none !important;
            /* word-break: keep-all; text-transform: none; font-size: 200%; line-height: 110%; */
            /* color: #060 !important; */
            /* color: #444 !important; */ /* grey from the wab page */
            font-weight: bold !important;
            -webkit-hyphens: none !important;
            -moz-hyphens: none !important;
            -ms-hyphens: none !important;
            hyphens: none !important;
            line-height: 110% !important;
        }
        .reveal .progress span {
            background-color: #444 !important;
        }
        /* predefined element positioning */
        .top {
            /*position: relative;*/
            top: 5%;
            height: 45%; /* is the height even needed? */
        }
        .bottom {
            height: 45%;
        }
        .ne {
            position: absolute;
            top: 5%;
            right: 5%;
            height: 45%;
            width: 45%;
        }
        .nw {
            position: absolute;
            top: 5%;
            left: 5%;
            height: 45%;
            width: 45%;
        }
        .se {
            position: absolute;
            bottom: 5%;
            right: 5%;
            height: 45%;
            width: 45%;
        }
        .sw {
            position: absolute;
            bottom: 5%;
            left: 5%;
            height: 45%;
            width: 45%;
        }

        /* classes for sections with predefined elements */
        /* using !important because, reveal styles are applied afterwards  */
        .right, .textimg > img, .textimg > video, .textimg > iframe, .imgtext > p, .imgtext > ul, .imgtext > ol, .imgtext > div {
            float: right;
            text-align: left;
            max-width: 47% !important;
        }
        .left, .imgtext > img, .imgtext > video, imgtext > iframe, .textimg > p, .textimg > ul, .textimg > ol, .textimg > div {
            float: left;
            text-align: left;
            max-width: 47% !important;
        }
        li > ul, li > ol {
            font-size: 85% !important;
            line-height: 110% !important;
        }
        .small {
            font-size: smaller !important;
            color: gray;
            margin: 0.1em !important;
        }
        .credit {
            font-size: small !important;
            color: gray;
            margin: 0.1em !important;
        }
        </style>
    </head>

    <body>


      <div class="reveal">


            <!-- Any section element inside of this container is displayed as a slide -->
            <div class="slides">

<!-- <section data-background= "img/intro_1.jpg" data-background-size="95%">
</section>
<section data-background= "img/intro_2.jpg" data-background-size="95%">
</section> -->

<section data-background= "img/intro_tech.jpg" data-background-size="95%">

      <aside class="notes">
      Geospatial computation, visualization and user interaction technologies —not only as
      tools but also as modes of operation and inquiry — can radically disrupt the study and design of our landscapes.
      With geospatial computation technologies, we can capture, in high precision, the urban Landscape structure and patterns.
      They also allow us to rapidly simulate the impact of our interventions on a range of environmental factors from flow of water
      and pollution to spread of fire and pathogens and to flow of traffic and human.
      </aside>
</section>

<section data-background= "img/intro_tech2.jpg" data-background-size="95%">

      <aside class="notes">
      With virtual reality and immersive virtual environments (IVEs), we can better represent and communicate “real world”
      experiences of existing landscapes and simulate lifelike “what if” scenarios for the imagined landscapes. ]
      These technologies also allow us to rigorously measure cognitive and affective responses to landscape experience.
      </aside>

</section>

<section data-background= "img/intro_tech3.jpg" data-background-size="95%">

      <aside class="notes">
      Advanced interaction technologies such as Tangible user interfaces, provide intuitive and inclusive access to
      these highly specialized knowledge and tools. By doing so they break the knowledge barriers and disciplinary divides between designers, scientists,
      and public allowing them to collaboratively design and decide about the future of their landscapes.
      </aside>

</section>

<section data-background="white">
        <h3> Coupling geospatial computation, virtual
        reality, and tangible interaction to improve landscape </br> design and research </h3>
        <br>
        <br>
        <h6> Payam Tabrizian </h6>
        <h6 style = "color : grey"> October 2018 </h6>
<ul>
<br><br>
  <table width="110%">
        <col width="6%">
        <col width="15%">
        <col width="6%">
        <col width="15%">
        <col width="6%">
        <col width="15%">

        <tr style = "font-size:.7em ; height: 1px">
          <td><img src="img/perver.jpg" style="width: 100%"></td>
          <td style = "border-bottom: 0px; vertical-align: middle">Perver Baran <br>
                                                                   Co-chair<br>
                                                                   <highlight style="background-color: lightgray ; font-size:.7em">Design</highlight>

          <td><img src="img/ross.jpg" style="width: 100%"></td>
          <td style = "border-bottom: 0px; vertical-align: middle">Ross Meentemeyer<br>
                                                                   Co-chair<br>
                                                                   <highlight style="background-color: #b4b4b4 ; font-size:.7em">Geospatial Analytics</highlight>

          <td><img src="img/helena.jpg" style="width: 100%"></td>
          <td td style = "border-bottom: 0px; vertical-align: middle">Helena  Mitasova <br>
                                                                      Commitee member <br>
                                                                      <highlight style="background-color: #b4b4b4; font-size:.7em">Geospatial Analytics</highlight></font>


        </tr>

        <tr style = "font-size:.7em ; height: 1px">

          <td style = "border-bottom: 0px"><img src="img/chris.jpg" style="width: 100% "></td>
          <td style = "border-bottom: 0px; vertical-align: middle">Christopher Mayhorn <br>
                                                                   Minor representative<br>
                                                                   <highlight style="background-color: #eeeeee; font-size:.7em">Cognitive sciences</highlight>

          <td style = "border-bottom: 0px"><img src="img/andy.png" style="width: 100%"></td>
          <td style = "border-bottom: 0px; vertical-align: middle">Andrew Fox<br>
                                                                   Commitee member<br>
                                                                   <highlight style="background-color: lightgray ; font-size:.7em">Design</highlight>
          <td style = "border-bottom: 0px"><img src="img/deni.jpg" style="width: 100%"></td>
          <td style = "border-bottom: 0px; vertical-align: middle">Deni Ruggeri <br>
                                                                   Commitee member<br>
                                                                   <highlight style="background-color: lightgray ; font-size:.7em">Design</highlight>


        </tr>

        <aside class="notes">
        In my PhD, I explore methodologies that leverage and integrate these technologies with aims to enhance the research and
        design of the urban landscape. Needless to say, this relies on the expertise of my co-chairs and committee members
        from Design and Landscape architecture, Geospatial analytics and cognitive sciences.
        </aside>

</table>



</section>

  <section data-background="white">
  <!-- <h3> Outline</h3><br> -->
    <ul>

        <p2>Chapter 1: Introduction</p2><br>
          <br>
        <p2 style = "margin-left:12%"><strong>Modeling experiential qualities of urban landscape with viewscape analysis and IVE</strong>
          <hr>

        <highlight style = "background-color:#eeeeee">Chapter 2:</highlight>
        Modeling visual characteristic of urban landscape with
          viewscape analysis of lidar surfaces and immersive virtual environments
          <br>

        <highlight style = "background-color:#eeeeee">Chapter 3:</highlight>
          Restorative viewscapes: spatial mapping of urban landscape’s restorative
          potential using viewscape modeling and photorealistic immersive virtual environments</p2>

          <br><br>
        <p2 style = "margin-left:12%"><strong>Realtime 3D modeling and immersion with geospatial data and tangible interaction </strong>
          <hr>
        <highlight style = "background-color:lightgray">Chapter 4:</highlight>
        Realtime modeling, rendering and VR with GIS and tangible interaction<br>

        <highlight style = "background-color:lightgray">Chapter 5:</highlight>
         Tangible immersion for ecological design </p2>
          <br><br>
        <p2>Chapter 6: Conclusion </p2>

    </ul>

        <aside class="notes">
        I specifically propose two methods. The first method contributes to landscape assessment and
        combines geospatial analysis and immersive virtual environments to model experiential qualities of urban environments.
        The second method integrates geospatial computation, automated 3D visualization and Tangible interaction to make design process more intuitive, efficient,
        accessible and collaborative. These two methods are described in four chapters and four articles.
        </aside>

  </section>


<!-- _______________ Chapter 2. Viewscape Experiment ________________-->

              <!-- --SLIDE 1-- intro-->
<section data-background="https://github.com/ptabriz/presentation_viewscape/raw/master/img/anim.gif" data-background-size="200" >
<br/><br/>
<br/><br/><br/>

<h4 class="shadow">Modeling human experience of urban landscape</h4><h4 class="shadow" style="font-size:1.3em"> through viewscape analysis of lidar and immersive virtual environments survey </h4>

<br/>
<br/>
<h4>Chapter 2 & Chapter 3<h4>

</section>


<section data-background="white">


      <h3> Landscape experiential quality assessment </h3>
      <ul>
        <li> Study of landscape visual charactristics and aesthetic and affective outcomes.
        <li> Subjective "Perception-based" approach
          <ul>
              <li> Use of photographs or in-situ surveys
              <li> Reliable, expensive, time-consuming, non-spatial, difficult to apply to large regions
          </ul>
        <li> Objective, "Expert-based" approach <reference> (Wilson et al., 2008; Vukomanovic et al. 2018)</reference> </li>
          <ul>
              <li> GIS-based assessment, using landscape perception theories
              <li> Allow for spatial mapping
              <li> Expert based, does not reflect community perceptions
          </ul>


      <aside class="notes">

        Landscape experiential or visual quality assessment refers to study of landscape in
        relation to its visual characteristics and aesthetic and experiential values.
        Visual characteristics such as naturalness, openness, harmony and complexity of environments in one way or another
        shape our aesthetics judgements as well as sense of safety, restorativeness, fascination, and so on.
        These experiences has a direct effect on our psychological well-being, and our motivation to visit, reside,
        or even maintain a landscape. So it is important to identify the visual characteristics and experiential values of existing landscapes,
        and to understand how landscape design can promote positive experiences.

        Study of landscape experience can be subjective or objective, the subjective or perception based approach uses in situ survey,
        photographs or simulated environments to understand the relationships between landscape pattern and structure and human perceptions.
        Because pictures represent landscape the way we perceive it real life they have a high reliability.
        But they are essentially non-spatial which makes them difficult for mapping purposes.

        The objective, or expert approach uses spatial analysis of GIS data to compute metrics for landscape structure and
        pattern and estimate landscape quality based on attributes such as openness, harmony, etc. While this approach allows for spatial mapping,
        it relies on the assessor’s judgement and does not reflect the experiences from the eye of the beholder.

      </aside>
      </section>

  <!-- <section data-background="white">
  <h3> Viewscape analysis </h3>
  <ul>
    <li> Analysis of composition and configuration of human visual domain on landscape (Wilson et al., 2008; Vukomanovic et al. 2018)</reference> </li>
    <li> Lidar technology
    <li> Immersive virtual enviornments
  </ul>

  <p style="float: left; font-size: 20pt; text-align: center; width: 46%; margin-right: .5%; margin-top: 3.5em;">
    <img src="https://github.com/ptabriz/presentation_viewscape/raw/master/img/viewshed-profile.png" style="width: 100%">
    <small> viewshed concept shown in profile </small>
  </p>
   <p style="float: left; font-size: 20pt; text-align: center; width: 46%; margin-right:.5%; margin-top: 1em;">
      <img src="https://github.com/ptabriz/presentation_viewscape/raw/master/img/binary-viewshed.png" style="width: 100%">
      <small> viewshed map computed on digital elevation model </small>
  </p>


  <aside class="notes">



  The analysis usually starts with computing the visible area from a 3D surface model using line-of sight analysis which is called viewshed map.
  The viewshed can be combined with landcover to estimate the composition of the visible surface ?
  Landscape structure analysis and spatial statisics can also show the configuration of the space such as
  is it wide or narrow vista ? how far one can see ?
  how diverse is the visible landscape and so forth ?


  </aside> -->

              <!-- </section>

              <section data-background="white">
              <h3> Landscape Assessment </h3>
              <ul>
                <li class="fragment fade-in"> Focus on regional and landscape scale, limited granularity for urban pattern and structure.
                <li class="fragment fade-in"> Limited field of view and representation validity of images
                <li class="fragment fade-in"> Limited to aesthetic preferences
                <li class="fragment fade-in"> LiDAR data and Immersive Virtual Environments (IVEs)
              </ul>

              <aside class="notes">

              However, majority of the studies utilizng this approach are focused on continental, regoinal and
              despite the monuting evidence on the impact of urban spaces on human health and ecosystem services,
              there is surprisingly there is limited studies on small-scale Urban environments.

              Also the geospatial data used is often very coarse and cannot be used to model
              fine-granied and heterogenous landcover

              Also, the GIS approach is objective and cannot adequately reflect the human perceptions

              And few studies that integrated subjective analysis are mainly
              focused on aesthetic appeal and overlook other important perceptions

              At the same advancemnet in remote sensing like LiDAR data can be used to create
              a high-resolution 3D model of the landscape.
              While visualization tool like Immersive virtual environment can be used to realistically
              represent on the-ground-situation, and elicit human perceptions.

              </aside>

              </section> -->

<section data-background="white">

    <h3> Aims and objectives </h3>
        <ul>

          <li> Develop and evaluate an integrated modeling approach to predict experiential attributes of urban evironments. </li>
          <li> Apply the method to generate a spatial model of perceived restoration potential for an urban site. </li>
      <br>
          <li> Compute human viewscapes from spatial data.
          <li> Improve the precision and detial of the spatial data.
          <li> Improve the representation of the 360<sup>o</sup> on-the-ground experience.

        </ul>
    <aside class="notes">

            “So the main aim of the was to develop an integrated approach for predicting visual
            characteristics and experiential qualities of urban environments.
            The second aim was to apply this method to model an experiential quality,
            particularly perceived restorative potential of an urban environments.”

            I specifically, chose perceived restoration because it is an important determinant of psychological well-being,
            and it has been the least studied cognitive measure. I also included perceived restoration potential defined as the degree
            to which someone perceived an environment as restorative. Most of them focus on aesthetic qualities and overlook other important
            experiences such as perceived safety, restorativeness, and so on. Understanding experiential qualities of urban environments can be
            particularly valuable because of the variety of stressors associated with urban living and limited availability and access to green space.


            Spousing these two paradigm has some challenges.
            First is to how to compute the landscape structure from the eye of the observer on the ground from remote sensing data and imagery.

            Second is to provide sufficient level of precision in geospatial data so it can capture the structure and pattern of
            environment with enough detail in the way that we perceive it on-the-ground. Most of the assessments
            mainly developed for landscape and regional scales and lack the precision and detail to account for fine-grained urban environments.

    </aside>


</section>


<section data-background="https://github.com/ptabriz/presentation_viewscape/raw/master/img/overview_3.jpg" data-background-size="80%">

    <h3 style="margin-bottom:80%"> Approach </h3>
      <!-- <p style=" text-align: left; font-size: 18pt;  width: 120%; margin-bottom: 0.5em">
    <img src="https://github.com/ptabriz/presentation_viewscape/raw/master/img/overview_3.jpg"> -->


    <aside class="notes">

      To achieve these objectives I am proposing a multi-step
      approach using viewscape modeling and immersive virtual environments.

      I utilise Viewscape analysis to link objective and subjective analysis of the landscape.
      Viewscape analysis a widely used GIS application for computing the landscape structure and pattern from the observer’s visual domain.
      The analysis delineates the 360 visible area from a given viewpoint on a 3D surface. We can then process this called viewshed map to compute the
      metrics related composition and configuration of landscape.
      Lidar remote sensing technology can be employed to develop high-resolution surface model and pattern
      that is more relevant to urban environment.

      I assess experiences using a survey of photorealistic immersive environments or IVEs.
      IVE technology represent the 360 viewscape with a high degree of presence and realism and can be programmed to
      rigorously capture perceptions during the immersion experience.

      I statistically compare the perceptions and viewscape metrics of representative viewpoints to examine whether and how reliably
      the spatial analysis can capture visual characteristics and predict the perceptions on the ground. In the next step, I extrapolate these
      relationships to entire study area to generate a predicted model of restoration potential, based on the human perceptions.

    </aside>

</section>


<section data-background= "white">

  <h3>Study area</h3>

  <p style="float: left; font-size: 18pt; text-align: center; width: 90%; margin-right: .5%; margin-bottom: 0.5em;">
    <img src="https://github.com/ptabriz/presentation_viewscape/raw/master/img/site_2.jpg" style="width: 100%">
        Dorothea Dix park, Raleigh, NC, 308 acres </p>

  <aside class="notes">
    For the purpose of this study I chose is the historic site Dorothia Dix park which is located here in Raleigh/North Carolina.
    I chose this site for two reasons. First, it has a very diverse land cover including natural and landscaping vegetation and historic and administrative buildings.

  </aside>

</section>


<section data-background= "https://github.com/ptabriz/presentation_viewscape/raw/master/img/news_dix.png" data-background-size="95%">

  <aside class="notes">
Also, the site is recently acquired by the city of Raleigh and planned to be the largest city park in Raleigh and a Landmark destination for North Carolina.
In the current planning phase, redevelopment are being formulated that will enhance recreational, leisure and educational potential of the urban park.
Providing spatial evidence of the restorative potential of the park can help in additionally incorporating cognitive benefits in park design.
  </aside>

</section>


<section data-background= "white">
<h3>Digital surface model (DSM)</h3>
  <ul>
  <p style="float: center; font-size: 18pt; text-align: center; width: 90%; margin-right: .5%; margin-bottom: 0.5em;">
    <img src="https://github.com/ptabriz/presentation_viewscape/raw/master/img/lidar.png" style="width: 100%">
      Airborne lidar (North Carolina QL2), Acquired Jan 11, 2015 (leaf-off),

      <aside class="notes">
      To obtain a high-resolution surface, I used North Carolina QL2 lidar data that has a
      3 points per square meter density and acquired in winter 2015.

      </aside>

</section>

<section data-background= "white">
 <h3>Digital surface model (DSM)</h3>

   <p style="float: center; font-size: 18pt; text-align: center; width: 90%; margin-right: .5%; margin-bottom: 0.5em;">
     <img src="https://github.com/ptabriz/presentation_viewscape/raw/master/img/lidar2.png" style="width: 100%">
       0.5m Digital surface model (DSM)

     <aside class="notes">
    Then I interpolated it to acquire a 0.5 meters surface model that represents buildings and vegetation.
     </aside>
</section>

<section data-background= "white">
 <h3> Tree obstruction error</h3>

   <p style="float: center; font-size: 18pt; text-align: center; width: 90%; margin-right: .5%; margin-bottom: 0.5em;">
     <img src="https://github.com/ptabriz/presentation_viewscape/raw/master/img/lidar3.png" style="width: 100%">
       0.5m Digital surface model (DSM)

     <aside class="notes">
    A well known limitation of interpolated surfaces is the way that trees are represented which can be a big
    source of error in visibility calculations.
     </aside>
</section>


<section data-background="white">

     <h3> Tree obstruction error</h3>


       <p style="float: left; font-size: 20pt; text-align: center; width: 40%; margin-bottom: 0.5em;margin-left:10%">
         <img src="https://github.com/ptabriz/presentation_viewscape/raw/master/img/obstruction.png" style="width: 100%">
         Real-world situation
       </p>

        <p style="float: left; font-size: 20pt; text-align: center; width: 40%; margin-right:.5%; margin-bottom: 0.5em;">
           <img src="https://github.com/ptabriz/presentation_viewscape/raw/master/img/obstruction2.png" style="width: 100%">
           Representation of trees in DSM
        </p>


      <aside class="notes">
      As an example, you can see that in the DSM trees are represented as solid walls
      that entirely obscure the under and through canopy visibility, which is very different in reality.
      </aside>

</section>

              <section  data-background="white">

                   <h3>Vegetation structure</h3>
                   <ul>
                    <p style="float: left; font-size: 18pt; text-align: center; width: 28%; margin-right: .5%; margin-bottom: 0.0em;">
                       <img src="https://github.com/ptabriz/presentation_viewscape/raw/master/img/structure_a.png" style="width: 100%"></p>

                    <p style="float: left; font-size: 18pt; text-align: center; width: 28%; margin-right:.5%; margin-bottom: 0.0em;">
                       <img src="https://github.com/ptabriz/presentation_viewscape/raw/master/img/structure_b.png" style="width: 100%"></p>

                    <p style="float: left; font-size: 18pt; text-align: center; width: 28%; margin-right: .5%; margin-bottom:0.0em;">
                         <img src="https://github.com/ptabriz/presentation_viewscape/raw/master/img/structure_c.png" style="width: 100%"></p>

                   <p style="float: left; font-size: 18pt; text-align: center; width: 28%; margin-right: .5%; margin-bottom: 0.0em;">
                     <img src="https://github.com/ptabriz/presentation_viewscape/raw/master/img/structure_1.png" style="width: 100%">Evergreen</p>

                    <p style="float: left; font-size: 18pt; text-align: center; width: 28%; margin-right:.5%; margin-bottom: 0.0em;">
                       <img src="https://github.com/ptabriz/presentation_viewscape/raw/master/img/structure_2.png" style="width: 100%">Evergreen + dense understory</p>

                    <p style="float: left; font-size: 18pt; text-align: center; width: 28%; margin-right: .5%; margin-bottom: 0.0em;">
                         <img src="https://github.com/ptabriz/presentation_viewscape/raw/master/img/structure_3.png" style="width: 100%">Deciduous stands</p>
                  </ul>

                  <aside class="notes">
                  However, a closer inspection of LiDAR points and site photos shows that deciduous specimen are all mostly affected by this
                  error whereas the mixed vegetation and evergreen trees are predominantly impermeable.
                  </aside>
              </section>


              <section data-transition="fade-out" data-background="white">
                <h3> Trunk obstruction modeling </h3>
                <p style="float: left; font-size: 20pt; text-align: center; width: 90%; margin-right: .5%; margin-bottom: 0.0em;">
                  <img src="https://github.com/ptabriz/presentation_viewscape/raw/master/img/dsm.png" style="width: 100%"><br>DSM</p>

                  <aside class="notes">
              To address this error I segmented the deciduous trees from
              the surface model and replaced them with their trunks through a process called trunk obstruction modeling.
                  </aside>
              </section>

              <section data-transition="fade-out" data-background="white">
                <h3> Trunk obstruction modeling </h3>
                <p style="float: left; font-size: 20pt; text-align: center; width: 90%; margin-right: .5%; margin-bottom: 0.0em;">
                  <img src="https://github.com/ptabriz/presentation_viewscape/raw/master/img/geomorphon.png" style="width: 100%"><br>Landform analysis (Geomorphon)</p>

                  <aside class="notes">
                  To segment the trees, I applied a landform detection algorithm called Geomorphons
                  to surface model to detect the treetops, which In this figure shown in dark brown dots.

                  </aside>
                  </section>

              <section data-transition="fade-in" data-background="white">
                <h3> Trunk obstruction modeling </h3>
                <p style="float: left; font-size: 20pt; text-align: center; width: 90%; margin-right: .5%; margin-bottom: 0.0em;">
                  <img src="https://github.com/ptabriz/presentation_viewscape/raw/master/img/peaks.png" style="width: 100%"><br>Extracted tree peaks</p>

              <aside class="notes">
                    Then, I extracted the peaks, and using the landcover data, I replaced the decidous trees with the tree trunks in the DSM.
                    The resulting surface model represents a more realistic representation of vegetation with deciduous trees shown as trunks and
                    the rest of the vegetation left intact.


              </aside>
              </section>


              <section data-transition="fade-in" data-background="white">
                <h3> Trunk obstruction modeling </h3>
                <p style="float: left; font-size: 20pt; text-align: center; width: 90%; margin-right: .5%; margin-bottom: 0.0em;">
                  <img src="https://github.com/ptabriz/presentation_viewscape/raw/master/img/trunk_replace.png" style="width: 100%"><br>DSM after trunk replacement</p>

                  <aside class="notes">
                   Then, I extracted the peaks, and using the land cover data, I replaced the deciduous trees with the tree trunks in the surface model.
                   The resulting surface model represents a more realistic representation of vegetation with deciduous trees shown as
                   trunks and the rest of the vegetation left intact.

                  </aside>

              </Section>


              <section data-background="white">
                  <h3> Trunk obstruction modeling </h3>
                   <ul>
                     <p style="float: left; font-size: 16pt; text-align: center; width: 43%; margin-left: 1.3%; margin-bottom: 0.0em;">
                       <img src="https://github.com/ptabriz/presentation_viewscape/raw/master/img/thin_view.jpg" style="width: 100%">
                       Viewscape before obstruction modeling </p>


                      <p style="float: left; font-size: 16pt; text-align: center; width: 43%; margin-left:1.355%; margin-bottom: 0.0em;">
                         <img src="https://github.com/ptabriz/presentation_viewscape/raw/master/img/thick_view.jpg" style="width: 100%">
                       Viewscape after obstruction modeling  </p>
                      <p style="float: left; font-size: 16pt; text-align: center; width: 90%; margin-left: 0%; margin-bottom:0.0em;">
                           <img src="https://github.com/ptabriz/presentation_viewscape/raw/master/img/extract.png" style="width: 100%">
                       Panoramic taken from viewscape point  </p>
                      </ul>

                      <aside class="notes">
                      The maps on the top show the viewscape analysis before and after the trunk obstruction modeling.
                      Comparison with the onsite panorama reveals
                      how much trunk modeling improved the visibility of between and underneath the deciduous canopies.

                      </aside>

              </section>


              <section data-background="white">

                   <h3> High resolution land cover</h3>
                   <ul>
                     <p style="float: left; font-size: 20pt; text-align: center; width: 32%; margin-right: .5%; margin-bottom: 0.5em;">
                       <img src="https://github.com/ptabriz/presentation_viewscape/raw/master/img/landcover_a.png" style="width: 100%">
                       Trees derived from lidar points</p>
                      <p style="float: left; font-size: 20pt; text-align: center; width: 32%; margin-right:.5%; margin-bottom: 0.5em;">
                         <img src="https://github.com/ptabriz/presentation_viewscape/raw/master/img/landcover_b.png" style="width: 100%">
                        Ground cover derived from supervised classification </p>
                      <p style="float: left; font-size: 20pt; text-align: center; width: 32%; margin-right: .5%; margin-bottom: 0.5em;">
                           <img src="https://github.com/ptabriz/presentation_viewscape/raw/master/img/landcover_c.png" style="width: 100%">
                           Roads and buildings derived from official vector data
              </p>
                    </ul>
                    <aside class="notes">
                      In the next step, I developed a detailed land cover to better represent the landscape pattern.
                      I combined Lidar vegetation points, with ground cover derived from image classification of multi-band imagery,
                      and buildings derived from city of Raleigh vector data.

                  </aside>
              </section>


              <section data-background="https://github.com/ptabriz/presentation_viewscape/raw/master/img/lancover_final.png" data-background-size="80%">
                <aside class="notes">
              This resulted in fairly detailed map with .5 m resolution and classification
              corresponding to the site’s existing land cover.
                </aside>
              </Section>


              <!-- <section data-background="white">
                <table style= "font-size: 16pt">
              <tr><th>Viewscape metrics</th><th>references</th></tr>
              <tr><td></td></tr>
              <tr><td><i><strong>Configuration metrics<i></td><td></td></tr>
              <tr><td>Extent (viewshed size)</td><td><reference>Tveit et al.(2009), Dramstad et al.(2006)
              <tr><td>Depth</td><td><reference>Ode et al.(2008), Fry et al.(2012), Tveit et al.(2009) </td></tr>
              <tr><td>Relief</td><td><reference>Tveit et al.(2009), Dramstad et al.(2006)</td></tr>
              <tr><td>Visible Horizontal surface</td><td><reference>Tveit et al.(2009), Stamps(2005)</td></tr>
              <tr><td>Viewdepth variation</td><td><reference>Sahraoui(2016)</td></tr>
              <tr><td>Skyline</td><td><reference>Sahraoui(2016)</td></tr>
              </td></tr>
              <tr><td>Shannon diversity index (SDI)</td><td><reference>Sahraoui(2016), Sang(2008), Ode et al.(2008)</td></tr>
              <tr><td>Mean Shape Index (MSI)</td><td><reference>Stamps(2005), Sahraoui(2016)
              </td></tr>
              <tr><td>Edge density (ED)</td><td><reference>Sang(2008), Ode et al.(2008), Fry et al.(2012)</td></tr>
              <tr><td>Number of patches (Nump)</td><td><reference>Sang(2008), Ode et al.(2008), Fry et al.(2012)</td></tr>


              <tr><td></td></tr>
              <tr><td><em><strong>Composition metrics<em></td><td></td></tr>
              <tr><td> % Visible landcover</td><td><reference>Ode et al.(2008), Sahraoui(2016)</td></tr>

                </table>
                <aside class="notes">
                I chose a set of configuration and composition metrics that are shown to be related to
                human perceptions and determinant of landscape characteristics.
                </aside>

              </section> -->

<section data-background="white">
    <h3>Viewscape computation</h3>
    <p style="float: left; font-size: 18pt; text-align: left; width: 90%; margin-bottom: 0.5em;">
    <img src="https://github.com/ptabriz/presentation_viewscape/raw/master/img/viewscape_method_2.png" style="width: 100%">

    <aside class="notes">

              This diagram shows the workflow for computing viewscape metrics for a single viewscape.

              In the first step computed the viewshed map to obtain the visible area, then I intersected the viewshed with the landcover to obtain the visible coverage. Using the surface model and bareground model, I dissected the viewshed to horizontal and vertical visibility maps.
              Based on these maps, I was able to compute the viewscape metrics. I chose a set of configuration and composition metrics that are shown to be related to human perceptions. Composition variables simply indicate who “what we see” in a view and indicate proportional presence of each land cover like buildings and vegetation in a viewpoint. Configuration metrics as the name suggest show the dimensions, and diversity of the view. For example, Extent indicates how wide is your view and depth indicates how far you can see. Relief shows the flatness or variability of the terrain. More complex metrics such as Shannon diversity index show how complex is your view and how many different land covers are present in your view.

              Through a python Code implemented in GRASS GIS I reiterated this
              process for the every 5 meters in the entire study area resulting in 39120 points.

    </aside>
</section>

<section data-background="white">
      <h2>Composition metrics</h2>

      <table class= "condensed" width="90%" style = "font-size:.6em">
            <col width="10%">
            <col width="15%">
            <col width="15%">
            <col width="15%">
            <col width="15%">
            <!-- <tr style = "font-size:.7em">
              <th style="text-align:center"> Metric </th>
              <th style="text-align:center"> Viewpoint 1 </th>
              <th style="text-align:center"> Viewpoint 2 </th>
              <th style="text-align:center"> Viewpoint 3 </th>
            </tr> -->
            <tr class= "border_bottom_solid border_top">
              <td style="text-align:left; vertical-align: middle"> Area photo <br><br><br><br><br><br><br> Viewscape </td>
              <td><img src="https://github.com/ptabriz/presentation_viewscape/raw/master/img/natural.png" style="width: 100%"></td>
              <td><img src="https://github.com/ptabriz/presentation_viewscape/raw/master/img/natural2.png" style="width: 100%"></td>
              <td><img src="https://github.com/ptabriz/presentation_viewscape/raw/master/img/natural3.png" style="width: 100%"></td>
              <td><img src="https://github.com/ptabriz/presentation_viewscape/raw/master/img/natural4.png" style="width: 100%"></td>
            </tr>


            <tr class = "center_text">

            <td style = "text-align:left"> Herbaceous </td>
            <td> 0 %  </td>
            <td> 0 %  </td>
            <td> 0 %  </td>
            <td><highlight class="dark"> 65 % </highlight> </td>
          </tr>

          <tr class = "center_text">

            <td style = "text-align:left"> Mixed</td>
            <td> 12 %  </td>
            <td> 0 %  </td>
            <td> 5 %  </td>
            <td> 12 %  </td>
          </tr>

          <tr class = "center_text">

            <td style = "text-align:left"> Evergreen</td>
            <td> 0 %  </td>
            <td> 0 %  </td>
            <td> 4 %  </td>
            <td><highlight class="dark">  8 % </highlight </td>
          </tr>

          <tr class = "center_text">

            <td style = "text-align:left"> Deciduous </td>
            <td> 0 %  </td>
            <td> 18 %  </td>
            <td> <highlight class = "dark"> 38 % </highlight </td>
            <td> 15 %  </td>
          </tr>


          <tr class = "center_text">

            <td style = "text-align:left"> Grassland </td>
            <td> 17 %  </td>
            <td> <highlight class = "dark"> 38 % </highlight </td>
            <td> 32 %  </td>
            <td> 0 %  </td>
          </tr>

          <tr class = "center_text">

            <td style = "text-align:left"> Paved roads </td>
            <td> <highlight class = "dark"> 48 %  </highlight</td>
            <td> 27 %  </td>
            <td> 14 %  </td>
            <td> 0 %  </td>
          </tr>

          <tr class = "center_text">

            <td style = "text-align:left"> Buildings </td>
            <td> <highlight class = "dark"> 25 %  </td>
            <td> 17 %  </td>
            <td> 5 %  </td>
            <td> 0 %  </td>
          </tr>

        </table>


        <aside class="notes">
      I pulled out some examples to show how the metrics can help classifying landscape characteristics.
      From left to right you can see four different viewscapes across the park and their composition metrics in the bottom.
      Comparison with the aerial image shows that as the proportion of the natural coverage increase
      and the built coverage decreases, the naturalness of the locations are seemingly increasing.
        </aside>
</section>


<section data-background="white">
    <h2>Configuration metrics</h2>

      <table class= "condensed" width="80%" style = "font-size:.5em">
          <col width="10%">
          <col width="15%">
          <col width="15%">
          <col width="15%">
          <col width="15%">
            <!-- <tr style = "font-size:.7em">
              <th style="text-align:center"> Metric </th>
              <th style="text-align:center"> Viewpoint 1 </th>
              <th style="text-align:center"> Viewpoint 2 </th>
              <th style="text-align:center"> Viewpoint 3 </th>
            </tr> -->
          <tr class = "border_bottom_solid border_top no_space">
            <td style="text-align:left; vertical-align: middle"> Viewscape image </td>
            <td><img src="img/ENV_210.PNG" style="width: 80%"></td>
            <td><img src="img/ENV_194.png" style="width: 80%"></td>
            <td><img src="img/ENV_196.png" style="width: 80%"></td>

          <tr class= "border_bottom_solid no_space">
            <td style="text-align:left; vertical-align: middle"> Viewscape map</td>
            <td><img src="https://github.com/ptabriz/presentation_viewscape/raw/master/img/complexity_210.PNG" style="width:80%"></td>
            <td><img src="https://github.com/ptabriz/presentation_viewscape/raw/master/img/complexity_194.png" style="width: 80%"></td>
            <td><img src="https://github.com/ptabriz/presentation_viewscape/raw/master/img/complexity_196.png" style="width: 80%"></td>
          </tr>

          <tr class = "center_text">

            <td style = "text-align:left"> Extent (m<sup>2</sup>) </td>
            <td><highlight class="dark"> 185,000  </highlight>  </td>
            <td> 50,710   </td>
            <td> 1072   </td>
          </tr>

          <tr class = "center_text">
            <td style = "text-align:left"> releif (m)</td>

              <td><highlight class="dark">7.12</highlight></td>
              <td>4.76</td>
              <td>2.11</td>
          </tr>

          <tr class = "center_text">
            <td style = "text-align:left"> Vdepth (m)</td>
              <td>17.3</td>
              <td><highlight class="dark">29.90</highlight></td>
              <td>5.29</td>
          </tr>

          <tr class = "center_text">
            <td style = "text-align:left"> SDI </td>
              <td>1.11</td>
              <td><highlight class="dark">1.51</highlight></td>
              <td>1.49</td>
          </tr>

          <tr class = "center_text">
            <td style = "text-align:left"> MSI </td>
              <td>39.1</td>
              <td><highlight class="dark">63.48</highlight></td>
              <td>19.7</td>
          </tr>


          <tr class = "center_text">
            <td style = "text-align:left"> Nump </td>
              <td>3700</td>
              <td><highlight class="dark">4168</highlight></td>
              <td>642</td>
          </tr>


          <tr class = "center_text">
            <td style = "text-align:left"> Depth (m) </td>
              <td>538</td>
              <td>1293</td>
              <td>305</td>
          </tr>
      </table>

              <aside class="notes">
              From left to right you can see a how small and large viewscapes are charactrized <body>
              extent and depth parameters, and the complex and coherent viewscapes can be distinguished by Shannon Diversity index,
              Mean shape index, and patch number.
              </aside>
              </section>


<section data-background="https://github.com/ptabriz/presentation_viewscape/raw/master/img/mapping_objective.jpg" data-background-size="53%">
          <h2 style= "margin-bottom:84%">Maps of viewscape metrics</h2>

          <aside class="notes">
          I then applied all the viewscape metrics to their corresponding viewpoints to
          examine the spatial distribution of each characteristics across the park.


          <p style="float:center; font-size: 18pt; text-align: center; width: 80%; margin-left: 7em">
              <!-- <img src="https://github.com/ptabriz/presentation_viewscape/raw/master/img/mapping_objective.jpg" style="width: 100%"> -->
          </aside>
</section>

<section data-background="https://github.com/ptabriz/presentation_viewscape/raw/master/img/ive_sample.png" data-background-size="65%">
          <h3 style= "margin-bottom:84%">IVE survey</h3>
          <aside class="notes">
          To acquire subjective evaluations of landscape, I drew a set of viewpoints that correspondeded with all
          possible landscape composition and configuration as calculated from the viewscape metric, and also have enough distance to spatially represent the park.
          This resulted in 24 viewpoints.
          </aside>
</section>

<section data-background="white">
          <h3>IVE image acquisition method</h3>
          <p style="float: left; font-size: 18pt; text-align: left; width: 100%; margin-bottom: 0.5em;">
            <img src="https://github.com/ptabriz/presentation_viewscape/raw/master/img/ive_method.png" style="width: 100%">
              Image aquisition &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Stiching and editing
              &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Cube mapping and wrapping

    <aside class="notes">
      At each point I took 360 imagery using a DSLR camera mounted on a gigapan robot,
      and stiched the images to acquire a high-resolution 24m pixels panorama.
    </aside>

</section>

              <!-- <section data-background="white">

                <script src="https://360player.io/static/dist/scripts/embed.js" async></script>

                <h3>Sample IVE images</h3>
                <br>
                <iframe src="https://360player.io/p/aJY8U3/" frameborder="0" width=800 height=240 allowfullscreen data-token="aJY8U3"></iframe>
                <br><br>


              <iframe src="https://360player.io/p/jwFc2d/" frameborder="1" width=800 height=240 allowfullscreen data-token="jwFc2d"></iframe>


              </section>

              <section data-background="white">

                <h3>Sample IVE images</h3>
                <br>

                <iframe src="https://360player.io/p/UUPC9m/" frameborder="0" width=800 height=240 allowfullscreen data-token="UUPC9m"></iframe>
                <br><br>
                <iframe src="https://360player.io/p/rMeBHm/" frameborder="0" width=800 height=240 allowfullscreen data-token="rMeBHm"></iframe>

              </section> -->

<section data-background="img/panoramas.png" data-background-size="95%">

    <aside class="notes">
  These images provide a taste of
  diffrent characteristic of the site and sampled locations.
    </aside>
</section>

                  <section data-background="white">
                  <h3> IVE survey</h3>


                  <ul>

                  <li> <strong>Sample:</strong> 102 total undergraduate students, park recreation tourism management
                  <li> <strong>Design:</strong> Repeated measure (24 randomized trials)
                  <li> <strong>Response measures </strong>

                 </ul>
                 <br><br>

                      <table style = "float: left; margin-left: 1em ;font-size: 14pt">

                        <col width="5%">
                        <col width="15%">
                        <col width="15%">

                        <tr><th>item</th><th style= "font-size: 12pt">question</th><th style= "font-size: 12pt">reference</th></tr>

                        <tr><td>Visual access</td><td style= "font-size: 12pt; color: grey">How well can you see all parts of this setting without having your view blocked or interfered with?
                        </td><td><reference>Herzog & Kutzli, 2002</td></tr>

                        <tr><td>Complexity</td><td><reference style= "font-size: 12pt">I perceive this environments as . . . Simple=0, Complex=10 </td>
                        </td><td><reference></td></tr>

                        <tr><td>Naturalness</td><td><reference style= "font-size: 12pt">I perceive this environment as … Not natural = 0 , Natural =10 </td>
                        </td><td><reference>Marselle, Irvine, Lorenzo-Arribas, & Warber, 2015</td></tr>

                        <!-- <tr><td>Fascination</td><td><reference style= "font-size: 12pt">There is much to explore and discover here
                        </td><td><reference>Hartig, Korpela, Evans, & Gärling, 1996</td></tr> -->
<!--
                        <tr><td>Being Away</td><td><reference style= "font-size: 12pt">Spending time here gives me a good break from my day-to-day routine
                        </td><td><reference>Lindal & Hartig, 2013</td></tr> -->

                        <!-- <tr><td>Coherence</td><td><reference style= "font-size: 12pt">There is much to explore and discover here
                        </td><td><reference>Pals, Steg, Dontje, Siero, & van der Zee, 2014</td></tr> -->

                        <tr><td>Restorativeness</td><td><reference style= "font-size: 12pt">I would be able to rest and recover my ability to focus in this environment
                          </td><td><reference>Lindal & Hartig, 2013</td></tr>

                        <!-- <tr><td>Preference</td><td><reference style= "font-size: 12pt">I like this environment </td>
                          </td><td><reference>Nordh, Hartig, Hagerhall, & Fry, 2009</td></tr> -->
                      </table>



                  <aside class="notes">
                To gather people's perception of these 24 IVEs I conducted a lab-based repeated measure experiment with 102 undergraduate students.
                They for each IVE they responded to 10 perception measures 4 of which I have included in the dissertation report and presenting here.
                Those include more direct measures such as of perception of visual access or perception of naturalness and complexity.
                I also included perceived restoration potential defined as the degree to which someone perceived an envrioment as restorative.



                  </aside>
              </section>

              <section data-background="white">
                 <h3> Survey procedure</h3>

                    <img src="img/arrow_timline.jpg" style="width: 100%; margin-bottom: 0px">
                 <!-- <ul>
                   <li> Nature attitude survey (1 month prior to lab experiment)
                   <li> Briefing and calibration ~ 5 min
                   <li> Warmup (2 scenes, presence, realism) ~ 5 min
                   <li> Fatigue scenario - 2 min
                   <li> 12 trials > 2 min recess > 12 trials ~ 25 min
                   <li> Demographics (age, race, gender, major) and familiarity ~ 5 min
                 </ul> -->

                <video data-autoplay class="stretch"  src="https://github.com/ptabriz/presentation_viewscape/raw/master/img/video_restorativeness.mp4" frameborder="5" width="50%" loop="loop" style="width: 100%; border: 2px dashed darkgrey; margin-top: 0px">
              </video>

              <aside class="notes">

            I programmed the IVE representation and survey collection procedure in a VR development software.
            Each participant experienced a random presentation of the 24 immersive scenes and rated each on
            the perception measures with a 2 minutes break in between.

              </aside>
              </section>

              <!-- <section data-background="white">
                <h3>Data analysis (Chapter 2)</h3>

                <p style="float: left; font-size: 18pt; text-align: center; width: 80%; margin-left: 7em; margin-bottom: 0.5em;">
              <img src="https://github.com/ptabriz/presentation_viewscape/raw/master/img/Connection_3.jpg" style="width: 100%">

                <aside class="notes">
                I geolocated the points and for each point I took 360 imagery using a DSLR camera mounted on a gigapan robot,
                and stiched the images to acquire a high-reolution panorama.
                </aside>
              </section> -->

              <section data-background="white">
                <h3>Perceived visual charactristic</h3>

              <table style= "font-size: 16pt">
                <tr><th>Response variable</th><th>R2 adjusted</th><th>Significant independent variable</th></tr>
                <tr><td>Perceived Visual access</td><td>0.64</td><td style= "font-size: 14pt">Extent &uarr;***, Depth&uarr;**, Relief&darr;***, Vdepth var&darr;***, Building&darr;***, Paved&uarr;** , Deciduous&uarr;**, Building&darr;*** , Nump&darr;*** </td></tr>
                <tr><td>Perceived Complexity</td><td>0.42</td> <td style= "font-size: 14pt"> SDI *** &uarr;, Relief **&uarr;, Depth** &darr;, ED***&uarr;, Nump***&uarr;, Building**&uarr;</td></tr></tr>
                <tr><td>Perceived Naturalness</td><td>0.62</td><td style= "font-size: 14pt">Relief &uarr;***, Deciduous &uarr;**, Mixed&uarr;***, Herbaceous&uarr;***, Building&darr;***, Nump&uarr;**</td></tr>
                <tr><td>Perceived Restoration potential</td><td>0.72</td><td style= "font-size: 14pt">Extent &uarr;***,  Depth &darr;***, Relief &darr; ***, SDI&darr;***, Skyline&uarr;***, Deciduous &uarr;**, Mixed&uarr;***, Herbaceous&uarr;***, Building&darr;***, Paved&darr;**</td></tr>
                </table>

              <p style="float: left; font-size: 12pt; text-align: left; width: 100%; margin-top: 0.1em;">
              Generalized linear models for four response variables. Best model fit was determined by step-wise regression. <br>
              &uarr; : Positive association <br>&darr; : Nagative association <br>*p<0.05, ** p <0.01, *** p <0.001 <br>
              <i>Variables:</i> Vdepth_var = viewdepth variation, Nump = patch number, ED = edge density, MSI= mean shape index,
              SDI= shannon diversity index.

              </p>

              <aside class="notes">
                I used multiple linear regression analysis to examine
                the extent to which viewscape metrics explain the variations in perceptions of the 24 viewpoint.
                All independent variables (Viewscape metrics) were checked for collinearity.
                For each of the response variables, the best fit model was selected using stepwise selection method.
                The result seem promising and the specially for perceived restorativeness model which predicted 72% of the variations in perceived restorativeness potential.
                The results also revealed interesting relationships between viewscape characteristics and human perceptions.
                For example, the naturalness was mostly explained by presence of natural elements and absence of built elements,
                Whereas complexity was explained in large part by landcover heterogeneity and terrain roughness, and shape complexity.
              </aside>

              </section>


<!-- _______________ Chapter 3. Mapping________________-->

<section data-background="white">
  <h3>Perceived visual charactristic</h3>

<table style= "font-size: 16pt">
  <tr><th>Variable</th><th>coefficient</th><th>N.coefficient</th><th>Student t</th><th>sig</th></tr>
  <tr><td>Extent</td><td>&nbsp1.90E-05</td><td>&nbsp0.39</td><td>11.07</td><td>***</td></tr>
  <tr><td>Relief</td><td>-1.20E-01</td><td>-0.12</td><td>16.17</td><td>***</td></tr>
  <tr><td>Depth</td><td>-1.27E-03</td><td>-0.34</td><td>-6.06</td><td>***</td></tr>
  <tr><td>Skyline</td><td>&nbsp1.08E-01</td><td>&nbsp0.12</td><td>-17.10</td><td>***</td></tr>
  <tr><td>Vdepth_var</td><td>&nbsp7.18E-02</td><td>&nbsp0.27</td><td>&nbsp6.25</td><td>***</td></tr>
  <tr><td>SDI</td><td>-3.82E-01</td><td>-0.14</td><td>-1.92</td><td>**</td></tr>
  <tr><td>Building</td><td>-5.41E-02</td><td>-0.37</td><td>-5.38</td><td>***</td></tr>
  <tr><td>Paved</td><td>-6.14E-03</td><td>-0.03</td><td>-1.53</td><td>*</td></tr>
  <tr><td>Mixed</td><td>&nbsp1.07E-01</td><td>&nbsp0.66</td><td>&nbsp23.97</td><td>***</td></tr>
  <tr><td>Deciduous</td><td>&nbsp8.13E-02</td><td>&nbsp0.32</td><td>&nbsp16.47</td><td>***</td></tr>
  <tr><td>Herbaceous</td><td>&nbsp4.44E-02</td><td>&nbsp0.27</td><td>&nbsp20.30</td><td>***</td></tr>
  </table>

<p style="float: center; font-size: 14pt; text-align: left; width: 100%; margin-top: 0.1em;">
Generalized linear models perceived restoration potential. Best model fit was determined by step-wise regression.*p<0.05, ** p <0.01, *** p <0.001 <br>
<i>Variables:</i> Vdepth_var = viewdepth variation, Nump = patch number, ED = edge density, MSI= mean shape index,
SDI= shannon diversity index.

</p>

<aside class="notes">
  Looking closer at the perceived restorative model, the presence of lush vegetation such as
  herbaceous cover and mixed forest positively impacted perception of restoration potential
  meaning that natural areas in the park were in general perceived more positively.
  Both viewsheds with higher area and lower depth were rated positively indicating that areas with high level
  of prospect and cosy refuge spaces were perceived as highly restorative.

</aside>


</section>

<!-- <section data-background="white">
  <h3>Spatial mapping of perceptions</h2>
  <ul>
    <li> Landcover mapping. <reference> (Burkhard et al. 2009)</reference>
       <ul>
         <li> Less accurate <reference> (Van Zenten al. 2016)</reference>
       </ul>
    <li> Landscape configuration and stucture mapping <reference> (Van berkel,2014) </reference>
      <ul>
        <li> High-autocorrelation <reference>(Van Zenten al. 2016)</reference>
      </ul>
    <li> Viewscape configuration and composition mapping <reference>(Sahraoui et al. 2017) </reference>
  </ul>

  <br>

  <ul>
    <p style="float: left; font-size: 12pt; text-align: center; width: 32%; margin-top: 1.2em">
  <img src="https://github.com/ptabriz/presentation_viewscape/raw/master/img/mapping_features.jpg" style="width: 100%">
  Landcover mapping
  <p style="float: left; font-size: 12pt; text-align: center; width: 32%; margin-bottom: 0em">
  <img src="https://github.com/ptabriz/presentation_viewscape/raw/master/img/mapping_composition.jpg" style="width: 100%">
  Landscape composition and structure mapping
  <p style="float: left; font-size: 12pt; text-align:  center; width: 32%; margin-top: 1.75em">
  <img src="https://github.com/ptabriz/presentation_viewscape/raw/master/viewscape_mapping.png" style="width: 100%">
  Viewscape composition and structure mapping
  </ul>

</section> -->



<section data-background="white">
  <h2>Perceived restoration potential model</h2>

    <table  width = "100%" class= "condensed">
        <tr>
              <td style="text-align:center"><img src="https://github.com/ptabriz/presentation_viewscape/raw/master/img/restorative_map.jpg" style="width: 50%"></td>
        </tr>
        <tr>
              <td style="vertical-align:middle; text-align:left; font-size:14px; border-bottom: 0px">

              \[Y= 1.90\mathrm{e}{-5}(Extent) -1.20\mathrm{e}{-1}(Relief) -
              3.82\mathrm{e}{-1}(SDI) \quad + \quad ... \quad- 1.44\mathrm{e}{-2}(Evergreen) +
              4.44\mathrm{e}{-2}(Herbaceous) \]
            </td>
        </tr>
    </table>

    <aside class="notes">
    To achieve the second aim of the study, which was to acquire a spatial model of restoration potential.
    I applied the coefficients derived from viewscape model to all the other viewpoints across the study area.
    The resulting predictive map indicates hotspots for higher restoration likelihood and coldspots which are less likely to be perceived as restorative.
    The greenway, the natural, unpaved areas and rolling hills with scattered willow oaks were among those with highest restoration potential,
    whereas densely built areas showed the least likelihood of restoration.
    </aside>

</section>



<section data-background="white">

  <h3> Conclusion</h3>
  <ul>
      <li>  The proposed integrated methodology can be used to model experiential qualities of of urban environments.
      <li>  Several novel relationships between landscape structure and pattern and perceptions were identified.
      <li>  The method can be transferred to other applications.
      <li>  Photorealistic IVEs can be used as an efficient, easy-to-use and realistic method to represent and capture in-situ experiences.
  </ul>

      <aside class="notes">
      The findings of this study indicate that the integrated methodology can be used to model experience of urban environments.
      These spatial explicit information based on community perception can be useful for designers and planners to better understand the site qualities and
      make more targeted decisions for preservation or modification of certain areas.
      To my knowledge and integrated method that can be used to study urban landscape has not yet been developed before.

      At the same time the method can be used to investigate much needed evidence on the relationships between urban
      form and human experience as design guidelines for developing more appealing and restorative environments.
      My suggested protocol for developing high-resolution spatial data and improvements in vegetation modeling can
      potentially be useful for other research applications, such as visual impact of wind turbines, or forest clearing,
      or recreational and tourist attraction potential of forested landscapes.
      Finally, the Photorealistic VRs can be used as cost effective and efficient way to capture community
      perceptions of developing sites and create an experiential inventory of site quality before, during and after changes.
      </aside>

</section>

<section data-background="white">

    <h2>Publications</h2>

    <p3><highlight style = "background-color:#DEDEDE">Chapter 2.</highlight> Modeling visual characteristic of urban landscape with
      viewscape analysis of lidar surfaces and immersive virtual environments
      <br><br>Target Journal:<i> Computers, Environment and Urban Systems</i></p3>
<hr>

    <p3><highlight style = "background-color:#DEDEDE">Chapter 3.</highlight> Restorative viewscapes: spatial mapping of urban landscape’s restorative
      potential using viewscape modeling and photorealistic immersive virtual environments
      <br><br>Target Journal: <i>Landscape and Urban Planning</i></p3>



    <aside class="notes">
    Two manuscripts have been developed for this project soon which is soon to be submitted for review.
    The first paper focuses on describing the method and evaluating its capacity to explain visual characteristics of urban environments.
    The second manuscript, targeted for landscape and urban planning journal has a more applied perspective and focuses on development of
    the restoration potential map for the dix area.
    </aside>

</section>

<section data-background="white">

  <h3>Related contributions</h3>

<h2>Viewscape method</h2>
  <!-- <li> Viewscape analysis <br> -->
  <p3>Van Berkel, D., <strong>Tabrizian, P. </strong> , Dorning, M. A., Smart, L.,
       Newcomb, D., Mehaffey, M., … Meentemeyer, R. K., (2018)
       <a href = "https://www.sciencedirect.com/science/article/pii/S2212041617307714">
         Quantifying the visual-sensory landscape qualities that contribute to cultural ecosystem services using social media and LiDAR.
  </a><i> Ecosystem Services</i>, 31, Part C, pp. 326-335.

</p3>

<br>
  <p3>Van Berkel, D., Tieskens, T., <strong>Tabrizian, P.</strong>, Van Zanten, B., Smith, J., … M., Neale, A., and Verburg, P.
    National assessment of cultural ecosystem services: Leveraging social media to understand America’s most valued landscapes.
    <i>Nature Sustainability</i>, in preperation.</p3>
  <br>

<h2>IVE survey method</h2>

<p3><strong>Tabrizian, P. </strong>, Baran, P., Smith, W. R. & Meentemeyer, R. K. (2018), <a href = "https://www.sciencedirect.com/science/article/pii/S0272494418300124?via%3Dihub"> Exploring perceived restoration potential of urban green enclosure through
immersive virtual environments </a>, <i>Journal of Environmental Psychology </i>, 55.</p3>

  <p3>Baran, P., <strong>Tabrizian, P. </strong>, Zahi, J., Smith, J. W., Floyd, M. (2018) <a href = "https://ptabriz.github.io/publication.html">An exploratory study of perceived
    safety in a neighborhood park using immersive virtual environments</a>
    , <i>Journal of Urban Forestry and Urban Greening </i>, in press.</p3>


<aside class="notes">
Both Viewscape analysis workflow and perception survey has been developed as independent software that can be used in many research applications.
For example with Dr. Van Berkel, our colleague at EPA, we used our automated viewscape modeling with geotagged photos to model
cultural ecosystem services for the entire North Carolina. We are currently extending this investigation to develop a model for
the entirety of United States with more than 10 million viewpoints.
The VR survey software can also be used to conduct design perception surveys with immersive photos,
which we have used in several of our experiments with Dr. Baran,
a couple of which are published in Environmental Psychology and Urban Greening and Urban Forestry Journals.
</aside>

</section>


<section>
<h3>Related contributions</h3>


<h2>Presentation</h2>

<p3>Tabrizian, P., Baran, P., Mitasova, H., & Meentemeyer, R. K. (2018), <a href = "https://ptabriz.github.io/publication.html">Developing viewscape model for urban landscape using LiDAR and Immersive
Virtual Environments</a>,<i>US Regional Association of the International Association for Landscape Ecology (USIALE)</i>, Chicago, Il,
8-12 April.</p3>

<p3>Tabrizian, P., Petrasova, A., Petras, V., Mitasova, H.(2017). <a href = "https://ptabriz.github.io/publication.html">Using open-source tools and high-resolution geospatial data to estimate landscapes’
visual attributes.</a> <i>International conference for Free and Open Source Software for Geospatial (FOSS4G)</i>, Boston, MA, Aug 14-19.</p3>

<p3>Tabrizian, P., Baran, P. (2017). <a href = "https://ptabriz.github.io/publication.html">Immersive Virtual Environment Technology in Environmental Design Research: Experimental Methods and
Procedures</a>, <i> 48th environment and design research association (EDRA) annual conference </i>, Madison, Wisconsin, May 31- June 6.</p3>

<p3>Baran, P., Tabrizian, P. (2017).<a href = "https://ptabriz.github.io/publication.html"> Linking Immersive Virtual Environments to complement human perception research</a>, <i>NCGIS
conference</i>, Raleigh, North Carolina. 23-24 Feb.</p3>

<p3>Baran, P., Tabrizian, P. (2016).<a href = "https://ptabriz.github.io/publication.html"> Use of Immersive Virtual Environments in mapping perceived safety in a park </a>, <i>39th National
Recreation and Park Association (NRPA) Annual Conference</i>, St. Louis, Missouri, October 5–6.</p3>

<aside class="notes">


</aside>
</section>


<!-- _______________ Chapter 4. COUPLING________________-->

<!-- --SLIDE 1-- intro-->
<section data-background="
https://github.com/ncsu-geoforall-lab/tangible-landscape-talk/raw/master/img/slider_1.jpg"
data-background-size="100%">

<h4 class="shadow">Realtime 3D modeling and immersion with geospatial data and tangible interaction</h4>

  <br/>
  <br/>
<h4 class="shadow" style = "font-size:1.4em">Chapter 4 & Chapter 5<h4>
</section>

 <aside class="notes">

 In the previous chapter, we discussed how objective analysis and subjective can be integrated
 to advance landscape assessment. The second method aims to integrate these aspects into the design process.

 </aside>

</section>




<section data-background="white">

  <h3>The Science/Design divide </h3>
  <ul>
    <li> Effective Design and decision making require collaboration and public participation
    <li> Tools are different, specialized and complicated
    <li> Assessment of environmental/experiential trade-offs are difficult
    <li> Design problem solving process is iterative
  </ul>

<aside  class="notes">

But why do we need to blend experiential analysis and scientific inquiry into the design process.
As we all know, design and planning process today require collaboration between designers, engineers,and scientists to effectively address environmental
aspects such as flow of water, traffic, erosion, along with aesthetic and design considerations.
At the same time design scenarios should integrate public input and communicate tradeoffs the way the can understand and participate.

Designers use advanced tools such as CAD, 3D visualization and so on to draft the plans and represent design outcomes
in the way that clients and stakeholders can understand, while scientists and engineers work with spatial analysis and metrics to deliver the outcomes.
As these software and tools become more and more specialized, integration of scientific inquiry into iterative design process becomes more and more difficult.
These divides slows down the design process, and makes public collaboration and participation difficult.

</aside>
</section>


<section data-background="white">
<h3> Aims </h3>

<ul>
  <li> Develop an easy-to-use collaborative design tool that enables simultaneous
       assessment of experiential and spatial analysis in the iterative design process
  <li> Evalaute the prototype's functionality using a design case-study
</ul>

<aside  class="notes">
  With those questions in mind, I sought to develop a method that enables more intuitive interaction with design phenomenon and simultaneous
  access to spatial analysis and experiential analysis in each iteration of design.

</aside>

</section>

<section data-background="white">
<h3> Approach </h3>
<img class="stretch" src="https://github.com/ncsu-geoforall-lab/tangible-landscape-talk/raw/master/img/procedural_3D_modeling/approach_cycle.jpg">

  <aside  class="notes">
  My approach to achieving these aims is to leverage Tangible interaction,
  Geospatial Computation and 3D visualizations to enable simultaneous spatial and experiential assessment
  in each iteration of design. Tangible interfaces enable for more intuitive, and collaborative manipulation of spatial features.
  Geospatial computation at the same time provides the spatial analysis of the changing environments, while 3D visualizations supports
  human view representation of environments and aesthetic exploration.
  </aside>

</section>


<section data-background="white">
<h3> Approach </h3>
<img class="stretch" src="https://github.com/ncsu-geoforall-lab/tangible-landscape-talk/raw/master/img/procedural_3D_modeling/approach_cycle_2.jpg">

  <aside  class="notes">
  Realtime coupling linkage between Tangible interaction and geospatial analysis is already enabled thanks to Tangible Landscape system.
  My proposed addition links geospatial analysis with 3D modeling and rendering so that design scenarios can be rendered into
  perspective views and immersive scenes, in real time. Tools like 3D rendering and immersive virtual
  environment representing environments in a way that people experience everyday, leading to better understanding and engagement,
  and thus gives them more agency to participate in the design process.
  </aside>

</section>
<!--<section data-background="white">
<img class="stretch" src="https://github.com/ncsu-geoforall-lab/tangible-landscape-talk/raw/master/img/IVE+TL.jpg">
<div style="margin-left:7%;float:left;max-width:45% !important">
Tangible Interaction</div>
<div style="margin-right:5%;float:right;max-width:45% !important"> Realtime 3D rendering and immersion</div>

<aside class="notes">
  I will specifically discuss why and how I coupled Tangible landscape- a tangible interface for GIS and an immersive virtual environment and to make ecological design process more effective,
  and more imporantly how this technology can potentialy help bridging the gaps between experiential and ecological analysis of landscape.

</aside>
</section>-->

<!-- <section data-background="white">
  <h2>Scale</h2>
<img class="stretch" src="https://github.com/ncsu-geoforall-lab/tangible-landscape-talk/raw/master/img/flooding_secraf.jpg">

<div style="text-align:left">
<p2> &nbsp;&nbsp;&nbsp;&nbsp; In-situ view of inundated area &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;    	Surface inundation and flow model</p2>
</div>
    <aside class="notes">
      The first keyword comes to mind is perhaps the scale. As you have seen so far, Tangible Landscape represents the landscape as a projection-augmented model which is perceived in a bird’s-eye perspective.
      So we wanted to bring in the real-world human scale experience of spatial features and phenomenon.
    </aside>

</section>

  <section data-background="white">
  <h2>Interdiciplinary collaboration</h2>
<ul>
<img class="stretch" src="https://github.com/ncsu-geoforall-lab/tangible-landscape-talk/raw/master/img/colloboration.jpg" width=90%>
</ul>

<p>
  <p><small>Source: </i> <a href="http://www.pinsdaddy.com/landscape-architect-working_hGaVnJkWJ4Um0gbrE9EIycd9ZTWpIq0IPBDulPs%7CqEs/"> Pinsdaddy.com</a></small></p>
    <aside class="notes">
      Second is collobration. Landscape problems of today are complex and require interdiciplinary colloboration between designers,
      planners, engineering and scientist. So it is important for a successful spatail decision support system to accomodate visualization tools that designers work, and 3D rendering is the utmost impotance.
        </aside>
</section>

<section data-background="white">
  <h2>Participation</h2>
<ul>
<img class="stretch" src="https://github.com/ncsu-geoforall-lab/tangible-landscape-talk/raw/master/img/participation.png" width=90%>
</ul>
<p>
  <p><small>Source: </i> <a href="https://design.ncsu.edu/ah+sc/wp-content/uploads/2013/06/community-participation1-1024x587.png"> NC State design</a></small></p>
    <aside class="notes">
      Visualizing scenarios
        Tools like 3D rendering and immersive virtual environment representing enviornments in a way that poeple experience everyday, leading to better understanding and engamenent,
        and thus gives them more agency to participate in the design process.
</aside>
</section>

<section data-background="white">
  <h2>Aesthetics</h2>
<ul>
<img class="stretch" src="https://github.com/ncsu-geoforall-lab/tangible-landscape-talk/raw/master/img/aesthetics.jpg" width=90%>
</ul>
<p>
  <p><small> Landscape rendering produced by Tangible landscape</small></p>
    <aside class="notes">
      The last and perhaps the most important implication of rendering is enabling aesthetics.
      Abundant research has maded it crystal clear that people are not willing to accept and maintain the landscape they do not like, regardless of of its ecological value.
      So a ecologically sound and aesthetically pleasing landscape.
        </aside>
</section>

<section data-background="white">
<h2> Design process </h2>
<img class="stretch" src="https://github.com/ncsu-geoforall-lab/tangible-landscape-talk/raw/master/img/designprocess.png" width=50%>

<p>
  <p><small>Source: </i> <a href="https://www.tandfonline.com/doi/abs/10.1207/s15327051hci2101_4?journalCode=hciDesigning"> Visser (2010)</a></small></p>
    <aside class="notes">
    As you have seen so far, Tangible Landscape represents the landscape as a projection-augmented model which is perceived in a bird’s-eye perspective.
    So it is not capable of fully representing the real-world experience of geospatial feature or phenomenon, in the way that I perceive it in human view.
    </aside>
</section> -->

<section  data-background="white">
<h3>
    Tangible Landscape </h3>
<!-- <img height="350px" src="https://github.com/ptabriz/IDEO_presentation/raw/master/img/system_setup.png" > &nbsp;&nbsp;&nbsp; -->
<video data-autoplay width="700" height="380" controls muted>
  <source src="https://github.com/ptabriz/IDEO_presentation/raw/master/img/tl_video.mp4" type="video/mp4" data-background-video-loop="loop">
</video>
<ul>
  <li> Embodied and intuitive interaction, rapid sketching and geospatial feedback
  <li> Realtime streaming of GIS data
  <li> Robust open-source computation backend (GRASS GIS)



<aside class="notes">
A quick intro to Tangible landscape, it is a technology for tangible interaction with GIS developed here at the center for Geospatial analytics by
Anna Mitasova and collegues in Dr. Mitasova's lab. The technology uses a scanner to continously scan the changes in the elevation and pattern of a
a table model and sends it to GRASS GIS to run a varity of spatial analysis from water flow to fire spread and least cost path.
The resulting maps are then projected back to the model allowing users to rapidly sketch landscapes and get geospatial
feedback of their designs in near realtime.
</aside>

</section>

<section data-background="white">
<h3> Hardware setup </h3>
<img  style = "width:40%" src="https://github.com/ncsu-geoforall-lab/tangible-landscape-talk/raw/master/img/setup.jpg">

<aside class="notes">
For implementing the concept, we added a 3D modeling and game engine software, called blender to the tangible landscape setup with outputs
to a display and an immersive virtual reality headset. Blender is a free and open source program for modeling, rendering, simulation,
animation, and game design. Blender was perfectly suited to this application because it has an internal python-based IDE for programming
automated 3D modeling procedures and more importantly it has addons for importing GIS data. It also has a very efficient and realistic real-time
rendering capabilities with realtime output for head mounted displays.
</aside>

</section>

<section data-background="white">
<h3> Software architecture  </h3>
<img  style = "width:70%" src="https://github.com/ncsu-geoforall-lab/tangible-landscape-talk/raw/master/img/coupling_schema.jpg">

<aside class="notes">

On the software side, GRASS GIS and Blender are coupled through file-based communication,
a technique referred to as loose coupling. As user interacts with the tangible model or objects, GRASS GIS sends a copy of
the geo-coordinated information or simulation to a specified system directory. A monitoring module in blender scripting environment
that constantly watches the directory, identifies the type of incoming information,
and apply relevant operations needed to update the 3d scene.
We implemented this coupling and 3D modeling procedure as a plugin for Blender.

</aside>

</section>

<section data-background="white">

<table width="60%" style= "font-size: 16pt">
  <tr ><th style="text-align: center">Feature</th><th style="text-align: center">Import</th><th style="text-align: center">Model</th><th style="text-align: center">shading</th></tr>

  <tr class="border_bottom">
      <td style="vertical-align: middle"> Terrain raster </td>
      <td><img src="https://github.com/ncsu-geoforall-lab/tangible-landscape-talk/raw/master/img/procedural_3D_modeling/terrain_1.JPG"></td>
      <td><img src="https://github.com/ncsu-geoforall-lab/tangible-landscape-talk/raw/master/img/procedural_3D_modeling/terrain_2_1.JPG"></td>
      <td><img src="https://github.com/ncsu-geoforall-lab/tangible-landscape-talk/raw/master/img/procedural_3D_modeling/terrain_3.JPG"></td>

  </tr>

  <tr class="border_bottom">
      <td style="vertical-align: middle"> Water raster </td>
      <td><img src="https://github.com/ncsu-geoforall-lab/tangible-landscape-talk/raw/master/img/procedural_3D_modeling/water_1.jpg"></td>
      <td><img src="https://github.com/ncsu-geoforall-lab/tangible-landscape-talk/raw/master/img/procedural_3D_modeling/water_2.jpg"></td>
      <td><img src="https://github.com/ncsu-geoforall-lab/tangible-landscape-talk/raw/master/img/procedural_3D_modeling/water_3.jpg"></td>

  </tr>

  <tr class="border_bottom" >
    <td style="vertical-align: middle"> Patches polygons </td>
    <td><img src="https://github.com/ncsu-geoforall-lab/tangible-landscape-talk/raw/master/img/procedural_3D_modeling/tree_1.jpg"></td>
    <td><img src="https://github.com/ncsu-geoforall-lab/tangible-landscape-talk/raw/master/img/procedural_3D_modeling/tree_2.jpg"></td>
    <td><img src="https://github.com/ncsu-geoforall-lab/tangible-landscape-talk/raw/master/img/procedural_3D_modeling/tree_3.jpg"></td>

  </tr>

  <tr>
      <td style="vertical-align: middle"> Trail polyline </td>
      <td><img src="https://github.com/ncsu-geoforall-lab/tangible-landscape-talk/raw/master/img/procedural_3D_modeling/trail_1.jpg"></td>
      <td><img src="https://github.com/ncsu-geoforall-lab/tangible-landscape-talk/raw/master/img/procedural_3D_modeling/trail_2.jpg"></td>
      <td><img src="https://github.com/ncsu-geoforall-lab/tangible-landscape-talk/raw/master/img/procedural_3D_modeling/trail_3.jpg"></td>

  </tr>

  </table>

  <aside class="notes">
    Our implemented procedure for converting the GIS data to 3D rendering follows three general steps of importing the GIS features,
    turning them into 3D models and and apply materials and lighting. Different geospatial formats are supported,
    rasters can be used to communicate surfaces and terrains,
    polylines can be used for linear features such as waypoints and polygons can be used to
    delineate zone type objects such as patches of trees and urban blocks.
    Once imported specific modeling and rendering procedures is applied for each environmental features.
    For example terrain requires addition of side fringes and surface material whereas
    tree patches related should get populated with trees.


  </aside>

</section>

<section data-background="white">
<h3> Landform and water </h3>
<img class="stretch" src="https://github.com/ncsu-geoforall-lab/tangible-landscape-talk/raw/master/img/coupling_case.jpg">
<!-- <video data-autoplay  width="800" src="https://github.com/ncsu-geoforall-lab/tangible-landscape-talk/raw/master/img/water2.mp4" frameborder="0"></iframe> -->

   <aside class="notes">
     Here you can see that how tangible interaction, Geospatial analysis and 3D rendering can be used simultaneously to support landscape design activities.
     For example, users can sculpt the landscape to see the projection of water flow and accumulation simulations.
     You can see on the left side of the model we projected numeric feedback about the depth and surface area of the retained water.
     In this case both terrain and water data are sent to Blender to update the 3D model.

   </aside>

</section>

<!----SLIDE 19-- Plant species -->
<section data-background="white">
<h3> Vegetation  </h3>
<img class="stretch" src="https://github.com/ncsu-geoforall-lab/tangible-landscape-talk/raw/master/img/coupling_case2.jpg">

  <aside class="notes">

  To enable vegetation design, we leveraged color detection of the scanner and machine vision algorithms in GIS.
  They can either draw and cut their prefered shapes using scissors, or select from a library
  of cutouts with various shapes. Each color represents a landscape class, like decidous, evergreen etc.
  As user places the patches, GRASS GIS computes ecological analysis related landscape heterogeneity, biodiversity and complexity.
  We also project an estimated degree of polution remediation as a result of planting phyto remediating vegetation.
  After importing, Blender populates corresponding species in each patch based on a predefined spacing and density.

  While felt is an ideal material for placing patch-type objects on sand,
  color markers on dry and erase surfaces can be also used to alow freehand drawing.
  </aside>

</section>

<!-- --SLIDE 20-- Trails, features -->
<section data-background="white">
<h3> Paths </h3>
<img class="stretch" src="https://github.com/ncsu-geoforall-lab/tangible-landscape-talk/raw/master/img/coupling_case3.jpg">

 <aside class="notes">

   Additionally users can uses tangible objects, in this case wooden cubes to designtate a linear features, in this example a baord walk.
   As user inserts each of the chekpoints, Grass GIS, recalculetes and projects an optimal route using an algorithm that computes the least cost walking path.
   A profile of the road and the slope of the segments are projected as feedback (show them).
  The polyline feature is processed in Blender as a walktrough simulation that can viewed on screen or in HMD.

 </aside>

</section>

<!----SLIDE 21-- Human-views -->
<section data-background="white">
<h3> Cameras </h3>
<img class="stretch" src="https://github.com/ncsu-geoforall-lab/tangible-landscape-talk/raw/master/img/coupling_case7.jpg">

 <aside class="notes">
   The 3D model is interactive so anytime during the interaction users can freely navigate in the environment and explore diffrent vantage points using the mouse.
   But I wanted to keep that feature interactive as well. I used wooden marker with colored tip, that denotes the viewers location and direction of view.
   The feature is exported as a polyline feature. Once imported in blender,
   The scene camera is then relocated to the line’s starting point and the direction of view is aligned to the line’s endpoint.
 </aside>

</section>

<!----SLIDE 22 immersion-- -->
<section data-background="white">
  <h3> Immersion </h3>
   <video data-autoplay class="stretch"  src="https://github.com/ncsu-geoforall-lab/tangible-landscape-talk/raw/master/video/immersion.mp4" frameborder="0" loop="loop"></iframe>

<aside class="notes">
  Using a virtual reality addon, blender viewport is continuously displayed in both viewport and headmounted display,
  so users can pick up the headset and get immersed in their prefered views.
  One additional camera is also set to follow the imported trail feature to initiate a walkthrough animation if required.
</section>

<!----SLIDE 23 Realism-->

<!-- <section data-background="white">

<h2> Realism </h2>
<img class="stretch" src="https://github.com/ncsu-geoforall-lab/tangible-landscape-talk/raw/master/img/realism.jpg">

</section> -->

<section data-background="white">
<h3> Realism </h3>
<img class="stretch" src="https://github.com/ncsu-geoforall-lab/tangible-landscape-talk/raw/master/img/coupling_case5.jpg">
<p> Low poly rendering  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; High-poly style rendering

<aside class="notes">

Designers tend to work with different degrees of realism throughout the design process,
less realistic representations are suitable for massing and site planning whereas realistic presentations are utilized for the end product and clients.
We implemented a feature allowing designers to switch between different rendering modes anytime during the design process. As soon as user initiates select a different mode,
3D features including the sky to the trees are replaced with to a low-poly counterparts and cartoon materials.
As soon as user initiates select a different mode, 3D features including the sky to the trees are replaced with
to a low-poly counterparts and cartoon materials.

</aside>
</section>

<section data-background="white">
<h3> Realism </h3>
<img class="stretch" src="https://github.com/ptabriz/geodesign_with_blender/raw/master/img/render_hero_2.jpg">
<p4>cycles render engine, 5 minutes, 2 million pixels</p4><br><br>
<p> Realistic rendering </p>

<aside class="notes">
For end-products, scenes can be rendered with higher quality for printing and presentation purposes.
</aside>
</section>

<section data-background="white" data-transition="fade-out" >
<h3> Case-study</h3>

<table class= "condensed no_border" width="110%">
      <col width="64.5%">
      <col width="12%">
      <col width="12%">

      <!-- <tr  style = "font-size:.7em ; height: 1px">
      <td style= "border-bottom:0px; text-align:center">Tangible interaction</td>
        <td style= "border-bottom:0px; text-align:center">3D rendering</td>
      </tr> -->

      <tr class="no_border">

        <td style = "vertical-align:middle;border-bottom:0px">
          <img src="https://github.com/ncsu-geoforall-lab/tangible-landscape-talk/raw/master/img/design/site.png">
        </td>
          <td style= "border-bottom:0px">
              <table class= "condensed no_border">
                <tr class="no_border">
                  <td style= "border-bottom:0px"><img width="100%" src="img/mugshots/vasek_3.jpg"></td>
                </tr>
                <tr>
                  <td style= "border-bottom:0px"><img width="100%" src="img/mugshots/SAHAND_2.jpg"></td>
                </tr>

              </table>
          </td>

          <td style= "border-bottom:0px;  vertical-align:top">
              <table class= "condensed no_border">
                <tr class="no_border" style= "border-bottom:0px; vertical-align:top">
                  <td style= "border-bottom:0px; vertical-align:top"><p4><br>Participant A, PhD
                                                  <strong>Geospatial Scientist</strong><p4></td>
                </tr>
                <tr>
                  <td style= "border-bottom:0px; vertical-align:bottom"><p4 style= "vertical-align:bottom" ><br><br><br><br><br><br>Participant B, MLA
                                                  <strong>Landscape Architect<p4></strong></td>
                </tr>

              </table>
          </td>



    </tr>
</table>



<p>Spring Hill house site (48000 m<sup><small>2</small></sup>), Raleigh, NC</p>
<aside class="notes">
To test the functionality the system in the design process,
we conducted series of pilot user studies with landscape designers and engineers working together to design a park.
As an example, I am showing a design case-study collaboratively performed by a geospatial scientist and a landscape
architect to plan a small recreational site in Raleigh. Designer were tasked to design a recreational site with a
shelter and access to the parking area.

</aside>

</section>

<section data-background="white" data-transition="fade-in" >
<img class="stretch" src="https://github.com/ncsu-geoforall-lab/tangible-landscape-talk/raw/master/img/design/process.jpg">

<p>Changing landform and hydrology</p>

<aside class="notes">
  In the first step, they changed the landform to create a pond, and direct the road runoff to the redirect the
  road runoff to the pond. The used the excavation soil to create artificial mounds to
  buffer the site adjacent roads, and define an inviting view to and out of the site.
</aside>


</section>

<section data-background="white" data-transition="fade-in" >
<img class="stretch" src="https://github.com/ncsu-geoforall-lab/tangible-landscape-talk/raw/master/img/design/process2.jpg">
<p>Exploring views from the park site entrances</p>
<aside class="notes">

</aside>
Here you can see how they used the view markers to check the perspective views from the access roads.
</section>

<section data-background="white" data-transition="fade-in" >
<img class="stretch" src="https://github.com/ncsu-geoforall-lab/tangible-landscape-talk/raw/master/img/design/process3.jpg">
<p>Planting trees and siting the shelter </p>
<aside class="notes">
Then they planted four different plant species using felt pieces while exploring the biodiversity measures. Using a wooden marker,
they sited a shelter in the site that has best views to the pond and is protected by vegetation.
</aside>
</section>

<section data-background="white" data-transition="fade-in" >
<img class="stretch" src="https://github.com/ncsu-geoforall-lab/tangible-landscape-talk/raw/master/img/design/process4.jpg">
<p>Designing the trail and exploring views</p>

<aside class="notes">
  In the last step, they designed a scenic trail to connect the shelter to the parking lot,
  the trade-off was to minimize the trail slope while providing varied viewshed
  and views to the lake.

</aside>

</section>

<section data-background="white" data-transition="fade-in" >
<img class="stretch" src="https://github.com/ncsu-geoforall-lab/tangible-landscape-talk/raw/master/img/design/process7.jpg">
<p>Evaluation of design scenarios</p>

<aside class="notes">
They repeated the design process with another strategy. Each scenario tool around 10 minutes to develop and designers explored
multiple solutions in each step of the design. You can see that how these scenarios can be compared against each other to support decisions.
Spatial analysis such as landscape metrics, Site hydrology maps,
amount of runoff saved can be collaboratively balanced to create a landscape that works and is appealing.

</aside>
</section>

<section  data-background="white" >
<h3>Conclusion</h3>
<ul>
<li>  Can be used as a colloborative and efficient decision support system.
<li>  Potentially increases public engagement and participation in design and planning process.
<li>  Need for user studies (e.g., design cognition, user experience)
<li>  The framework for Realtime 3D rendering with geospatial data can be implemented as a desktop or web-based application.
<li>  The framework be transferred to other design and planning applications and scales.


</ul>

<aside class="notes">

  In conclusion, the proposed methodology allows designers to rapidly and iteratively develop designs,
  realistically visualize landscapes, and collaborate with others and public to better evaluate the tradeoff and win-win scenarios.
  The combination of Tangible interaction and automated spatial and experiential feedback can potentially break down the knowledge barriers
  allowing expert and non-expert to participate in the design process.
  However, all these claims should be empirically tested through multiple user studies and comparisons with typical
  design tools to see how interaction with system and its components influence the design process as well as collaboration, and participation.
  While the examples shown here were focused on Landscape design and related spatial analysis,
  the coupling framework can be extended to other planning and design applications.

</aside>
</section>

<section  data-background="white" >

  <table class= "condensed no_border" width="110%">
        <col width="25%">
        <col width="64.5%">

        <tr  style = "font-size:.7em ; height: 1px">
        <td style= "border-bottom:0px; text-align:center">Tangible interaction</td>
          <td style= "border-bottom:0px; text-align:center">3D rendering</td>
        </tr>

        <tr class="no_border">
            <td style= "border-bottom:0px">
                <table class= "condensed no_border">
                  <tr class="no_border">
                    <td style= "border-bottom:0px"><img width="100%" src="https://github.com/ncsu-geoforall-lab/tangible-landscape-talk/raw/master/img/applications/Sod_1.png"></td>
                  </tr>
                  <tr>
                    <td style= "border-bottom:0px"><img width="100%" src="https://github.com/ncsu-geoforall-lab/tangible-landscape-talk/raw/master/img/applications/Sod_anim.gif"></td>
                  </tr>

                </table>
            </td>
          <td style = "vertical-align:middle;border-bottom:0px"><img src="https://github.com/ncsu-geoforall-lab/tangible-landscape-talk/raw/master/img/futures/render_1.JPG"></td>

      </tr>
  </table>
  <p>
    3D rendering and animation with FUTURES model
  <br>
  <br>
      <p><small> Meentemeyer, et al. (2013), </i> <a href="https://www.tandfonline.com/doi/abs/10.1080/00045608.2012.707591">
        FUTURES: multilevel simulations of emerging urban–rural landscape structure using a stochastic patch-growing algorithm.</a>
      </small></p>

<aside class="notes">
  For example, we have used it to visualize an urban growth simulation called FUTURES
  Developed by Meentemeyer et al., FUTURES in 2013 is an open source urban growth model specifically designed to capture the spatial structure of development.
  FUTURES has been has been implemented in GRASS GIS and coupled with Tangible landscape.
  The case study is the Buncombe county region located in the North West of North Carolina with 660 Sq miles area.

</aside>



  <!-- <img width="25%" src="https://github.com/ncsu-geoforall-lab/tangible-landscape-talk/raw/master/img/applications/Sod_1.png">
  <img width="25%" src="https://github.com/ncsu-geoforall-lab/tangible-landscape-talk/raw/master/img/applications/Sod_2.png">
  <img width="25%" src="https://github.com/ncsu-geoforall-lab/tangible-landscape-talk/raw/master/img/applications/Sod_anim.gif"> -->

</section>


<section data-background="white">
<h2>Related publications</h2>

<p3><highlight style = "background-color:#90CB8F"> Chapter 4.</highlight>
<strong>Tabrizian, P.</strong>,
Harmon, B.,
Petrasova, A.,
Vaclav, P.,
Mitasova, H., &
Meentemeyer, R.K. (2017).
<a href = "http://papers.cumincad.org/cgi-bin/works/Show?acadia17_600">
Tangible immersion for ecological design</a>,
<i>Proceedings of the 37th Annual Conference of the Association
    for Computer Aided Design in Architecture (ACADIA)</i>, Cambridge, MA. pp. 600-609. </p3>

<p3><highlight style = "background-color:#eac95b"> Chapter 5.</highlight>
  <strong>Tabrizian, P.</strong><a href= "https://www.springer.com/us/book/9783319893020" >
  Realtime 3D modeling, VR and immersion </a>, In :
  Petrasova, A, Harmon, B., Petras, V, Tabrizian, P, Mitasova, H. <i> Tangible Modeling with Open Source GIS </i>, Springer, NY.
</p3>

<p3><highlight style = "background-color:#eac95b"> Chapter 5.</highlight>
  <strong>Tabrizian, P.</strong><a href= "https://www.springer.com/us/book/9783319893020" >
  Landscape design </a>, In :
  Petrasova, A, Harmon, B., Petras, V, Tabrizian, P, Mitasova, H. <i> Tangible Modeling with Open Source GIS </i>, Springer, NY.
</p3>

<p3><strong>Tabrizian, P.</strong>
  Petrasova, A.,
  Harmon, B.,
  Vaclav, P.,
  Mitasova, H.,
  & Meentemeyer,R. K. (2016).
  <a href= "https://www.springer.com/us/book/9783319893020" >Immersive Tangible Geospatial Modeling </a>, Proceedings
of 24th ACM SIGSPATIAL, <i>International Conference on Advances in Geographic Information Systems (ACM SIGSPATIAL)</i>, 2-6 Nov, Burlingame, CA.</p3>

<!-- <p3>Tateosian, L.,
<strong>Tabrizian, P. </strong>
   <a href = "https://www.researchgate.net/publication/320383098_Blending_tools_for_a_Smooth_Introduction_to_3D_Geovisualization">
  Blending tools for a Smooth Introduction to 3D Geovisualization.</a>
   <i>Proceedings of IEEE Visual Analytics Science
  and Technology (VAST) IEEE VIS </i>, Phoenix, Arizona.</p3> -->


  <h2>Software</h2>
  <p3>Realtime 3D rendering and immersion with Tangible Landscape.
    <a href = "https://github.com/ptabriz/3D_immersion_TL">Tangible Landscape Blender plugin.
  <br></p3></a>
  <br>


<aside class="notes">
This work has been published in three peer reviewed proceedings and two book chapters in Tangible Landscape book's second edition.
Also, the software is usable as a Blender plugin is open-source and publicly available.
It is currenlty being used in tandem with Tangible landscape in several research and academic instituions
including University of Georgia and University of Illions.
</aside>


</section>

<section data-background="white">
<h2>Related presentations</h2>
  <table class= "condensed no_border" width="110%">
        <col width="15%">
        <col width="35%">
        <col width="15%">
        <col width="35%">
  <tr>
    <td>
      <img src="img/paper_icons/iale-logo.png">
    </td>

    <td>
      <p4><strong>3D Visualization of Landscape Change Scenarios with Real-time Tangible Interaction.
      </strong> <i> US Regional Association of the International Association for Landscape Ecology (USIALE)</strong>, Chicago, Il, April 8-12
      2018</i></p4>
    </td>

    <td>
      <img src="img/paper_icons/acadia_logo.jpg">
    </td>

    <td style = "vertical-align : middle">
      <p4><strong>Tangible immersion for ecological design.
      </strong> <i> 37th Annual Conference of the Association for Computer Aided Design in Architecture (ACADIA)</strong>, Cambridge, MA , 2-4 November 2017</i></p4>
    </td>
  </tr>

  <tr>
    <td>
      <img src="img/paper_icons/foss4g_logo.png">
    </td>

    <td>
      <p4><strong>Coupling a geospatial Tangible User Interfaces (TUI) and an Immersive Virtual Environment
      (IVE) using using open-source geospatial and 3D modelling tools.
      </strong> <i> International conference for Free and Open Source Software for Geospatial, Boston, MA, Aug 14-19
      April.</i></p4>
    </td>

    <td  style = "vertical-align : middle">
      <img src="img/paper_icons/icc_logo.png">
    </td>

    <td style = "vertical-align : middle">
      <p4><strong>Tangible Landscape + VR: Moving along Reality-Virtuality gradient to deal with geospatial complexity.
      </strong> <i> 28th International cartographic conference (ICC), Washington D.C, July 2-7, 2017</i></p4>
    </td>
  </tr>


  <tr>
    <td style = "border-bottom:0px">
      <img src="img/paper_icons/edra_logo.png">
    </td>

    <td style = "border-bottom:0px">
      <p4><strong>Immersive Tangible Landscape modelling: a step towards the future for integrative
      ecological planning.
      </strong> <i> 48th environment and design research association (EDRA) annual conference, Madison, Wisconsin, May 31-
      June 6. 2017</i></p4>
    </td>

    <td style = "vertical-align : top; text-align:center; border-bottom:0px">
      <img src="img/paper_icons/sigspatial-logo.png"  width="50%">
    </td>

    <td style = "vertical-align:top; border-bottom:0px">
      <p4><strong>Immersive Tangible Modeling with Geospatial data.
      </strong> <i> International Conference on Advances in Geographic Information Systems (ACM SIGSPATIAL), San fransisco, CA, Oct 31-Nov 3, 2016</i></p4>
    </td>
  </tr>
</table>

<h2>Related workshops</h2>
<table class= "condensed no_border" width="110%">
      <col width="15%">
      <col width="35%">
      <col width="15%">
      <col width="35%">

<tr>
  <td style = "vertical-align :top; border-bottom:0px; text-align:center">
    <img src="img/paper_icons/cga_logo_harvard.png">
  </td>

  <td style = "vertical-align :middle; border-bottom:0px">
    <p4><strong>Real-time 3D modelling with Geospatial data</strong> <i>Harvard University Center for Geospatial Analysis (CGA)</strong>, Cambridge, US, Aug 2017
    2018</i></p4>
  </td>


    <td  style = "vertical-align : middle; border-bottom:0px">
      <img src="img/paper_icons/icc_logo.png">
    </td>

    <td style = "vertical-align : middle; border-bottom:0px">
      <p4><strong>3D Visualization of Geospatial Data With Blender and Sketchfab.
      </strong> <i> 28th International cartographic conference (ICC), Washington D.C, July 2-7, 2017</i></p4>
    </td>
</tr>

</table>


<aside class="notes">
This work has been published in two peer reviewed proceedings and two book chapters in Tangible Landscape book's second edition.

</aside>

</section>


<section data-background="white" >

<!-- <p2> This work relies on amazing support of ... </p2> -->
<h2> Aknowledgement </h2>
<table width="50%">
      <col width="4%">
      <col width="4%">
      <col width="4%">
      <col width="4%">
      <col width="4%">


      <tr style = "font-size:.7em ; height: 1px">

        <td><img src="img/mugshots/intellectual.jpg" style="width: 100%"></td>

        <td><img src="img/mugshots/anna.jpg" style="width: 100%"></td>

        <td><img src="img/mugshots/brendan.jpg" style="width: 100%"></td>

        <td><img src="img/mugshots/vasek.jpg" style="width: 100%"></td>

        <td><img src="img/mugshots/derek.jpg" style="width: 100%"></td>


      </tr>

      <tr style = "font-size:.7em ; height: 1px">


      <td><img src="img/mugshots/laura.jpg" style="width: 100%"></td>

      <td><img src="img/mugshots/jordan.jpg" style="width: 100%"></td>

      <td><img src="img/mugshots/garret.jpg" style="width: 100%"></td>

      <td><img src="img/mugshots/reza.jpg" style="width: 100%"></td>

      <td><img src="img/mugshots/saeed.jpg" style="width: 100%"></td>

      </tr>

      <tr style = "font-size:.7em ; height: 1px">

      <td><img src="img/mugshots/sahand.jpg" style="width: 100%"></td>
      <td><img src="img/mugshots/support.jpg" style="width: 100%"></td>

      <td><img src="img/mugshots/rachel.jpg" style="width: 100%"></td>
      <td><img src="img/mugshots/zach.jpg" style="width: 100%"></td>

      <td><img src="img/mugshots/emotional.jpg" style="width: 100%"></td>

      </tr>

      <tr style = "font-size:.7em ; height: 1px">

        <td><img src="img/mugshots/mooni.jpg" style="width: 100%"></td>

      <td><img src="img/mugshots/mom.jpg" style="width: 100%"></td>
      <td><img src="img/mugshots/dad.jpg" style="width: 100%"></td>

      <td><img src="img/mugshots/parnaz.jpg" style="width: 100%"></td>
      <td><img src="img/mugshots/hila.jpg" style="width: 100%"></td>

      </tr>


    </table>

    <aside class="notes">

    It goes without saying that none of these projects could have been realized without the awesome mentorship of my advisors,
    intellectual input of my colleagues, indefinite support of the CGA staff, and emotional support of my family.

     </aside>

</section>

<section data-background-video = "https://github.com/ncsu-geoforall-lab/tangible-landscape-talk/raw/master/video/case_study_video.mp4" frameborder="0">
  <h3 class="shadow"> </h3>
   <!--<video data-autoplay class="stretch"  src="https://github.com/ncsu-geoforall-lab/tangible-landscape-talk/raw/master/video/case_study_video.mp4" frameborder="0"></iframe>
   -->
   <aside class="notes">

 It goes without saying that none of these projects could have been realized without the awesome mentorship of my advisors,
 intellectual input of my colleagues, indefinite support of the CGA staff, and emotional support of my family.

    </aside>
</section>


<!-- <section data-background="white" >
<h3>Aesthetics, socio-cultural norms and action</h3>
  <p style="float: left; font-size: 18pt; text-align: left; width: 48%; margin-bottom: 0.5em;">
<img src="https://github.com/ptabriz/presentation_viewscape/raw/master/img/nassaur_1.png" style = "margin-right: 20px ;margin-top:50px ">
    <p style="float: left; font-size: 18pt; text-align: left; width: 48%; margin-bottom: 0.5em;">
<img src="https://github.com/ptabriz/presentation_viewscape/raw/master/img/nassaur_2.png" >

<br><br><br><br>
 <p><small> Gobster, P. H., Nassaur, J. I., Daniel, T. C., Fry, G. F. (2013), </i>
  <a href="https://www.semanticscholar.org/paper/The-shared-landscape%3A-what-does-aesthetics-have-to-Gobster-Nassauer/5d6a33b91e26b1d707805926b3ff54a6d6ad8d16">The shared landscape: what does aesthetics have to do with ecology?</a></small></p>

<aside class="notes">
In our next example, I scale up a bit to explore 3D rendering with a dynamic Urban-rural growth model, called FUTURES.
Developed by Meentemeyer et al., FUTURES in 2013 is an open source urban growth model specifically designed to capture the spatial structure of development.
FUTURES has been has been implemented in GRASS GIS and coupled with Tangible landscape.
The case study is the Buncombe county region located in the North West of North Carolina with 660 Sq miles area.

</aside>
</section> -->

<!-- <section data-background="white" >
<h3>PAR, co-creation and co-design</h3>
  <p style="float: left; font-size: 12pt; text-align: left; width: 32%; margin-top: 1em;">
    <img src="https://github.com/ptabriz/presentation_viewscape/raw/master/img/co_creation.jpg">
Nurses co-creating a concept for ideal <br>workflow  on a patient floor
  <p style="float: left; font-size: 12pt; text-align: left; width: 32%; margin-bottom: 0.5em;">
    <img src="https://github.com/ptabriz/presentation_viewscape/raw/master/img/co_design.jpg" >
    Nurses co-designing the ideal future patient room using a three dimensional
    toolkit for generative prototyping <reference>

      <p style="float: left; font-size: 12pt; text-align: left; width: 33.5%; margin-bottom: 0.5em; margin-right: 1em;">
        <img src="https://github.com/ptabriz/presentation_viewscape/raw/master/img/participatory.jpg" >
        Participatory 3D modeling of Tobago <reference>
<br><br><br><br><br><br>

<br><br><br><br>
 <p><small> Sanders, E.B.-N., (2006c) Nurse and patient participatory workshops for the NBBJ project:
Inpatient Tower Expansion for H. Lee Moffitt Cancer Center and Research Institute, Tampa,
Florida, USA.</p></small>

<p><small><a href= "http://participatorygis.blogspot.com/2012/10/participatory-3d-model-of-tobago-seen.html">
  Participatory 3D model of Tobago seen as time capsule</p></small>

<aside class="notes">
In our next example, I scale up a bit to explore 3D rendering with a dynamic Urban-rural growth model, called FUTURES.
Developed by Meentemeyer et al., FUTURES in 2013 is an open source urban growth model specifically designed to capture the spatial structure of development.
FUTURES has been has been implemented in GRASS GIS and coupled with Tangible landscape.
The case study is the Buncombe county region located in the North West of North Carolina with 660 Sq miles area.

</aside>
</section> -->


<!-- ________________ FUTURES __________________
<section data-background="white" data-transition="fade-in" >
<h3>Urban growth scenarios</h3>
<img width="32%" src="https://github.com/ncsu-geoforall-lab/tangible-landscape-talk/raw/master/img/applications/Sod_1.png">
<img width="32%" src="https://github.com/ncsu-geoforall-lab/tangible-landscape-talk/raw/master/img/applications/Sod_2.png">
<img width="32%" src="https://github.com/ncsu-geoforall-lab/tangible-landscape-talk/raw/master/img/applications/Sod_anim.gif">
<p>Simulation of urban growth scenarios with FUTURES model</p>
</section>

<section>
<h3>Urban growth scenarios</h3>

<iframe width="800" height="500" src="https://github.com/ncsu-geoforall-lab/tangible-landscape-talk/raw/master/https://www.youtube.com/embed/oFILb0En258" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>

<p>Simulation of urban growth scenarios with FUTURES model</p>
</section>

<section>
<img width="110%" src="https://github.com/ncsu-geoforall-lab/tangible-landscape-talk/raw/master/img/futures/render_1.JPG">
</ul>
<p>Rendering of the FUTURES simulation</p>
</section>

<section>
<img width="110%" src="https://github.com/ncsu-geoforall-lab/tangible-landscape-talk/raw/master/img/futures/night_render.JPG">
</ul>
<p>Night-time rendering of the FUTURES simulation</p>
</section>

<section>
  <h3> Road map</h3>
<img width="65%" src="https://github.com/ncsu-geoforall-lab/tangible-landscape-talk/raw/master/img/futures/typology_1.jpg">
</ul>
<p>User defined or simulated layout and typology</p>
<p><small>Source: </i> <a href="https://a-project.co.uk/2014/12/03/field-2-_-urban-typologies/"> a-project</a></small></p>

</section>

<section>
  <h3> Road map</h3>
<img width="50%" src="https://github.com/ncsu-geoforall-lab/tangible-landscape-talk/raw/master/img/futures/LOD.png">
</ul>
<p>Level of Detail management (LOD)</p>
<p><small>Source: </i> <a href="https://www.sciencedirect.com/science/article/pii/S0198971516300436?via%3Dihub#!"> Biljecki et al.(2016)</a></small></p>

</section>

<section>
  <h3> Road map</h3>
<img width="80%" src="https://github.com/ncsu-geoforall-lab/tangible-landscape-talk/raw/master/img/futures/enhanced_textures.jpg">
</ul>
<p>Enhanced textures</p>
<p><small>Source: </i> <a href="https://www.sciencedirect.com/science/article/pii/S0198971516300436?via%3Dihub#!"> Biljecki et al.(2016)</a></small></p>

</section>


<section>
<h2>Open source</h2>

<p>Tangible Landscape plugin for GRASS GIS <br>
    <a href="https://github.com/tangible-landscape/grass-tangible-landscape">
        github.com/tangible-landscape/grass-tangible-landscape
    </a></p>
    <p>Tangible Landscape plugin for Blender <br>
        <a href="https://github.com/tangible-landscape/tangible-landscape-immersive-extension">
            github.com/tangible-landscape/tangible-landscape-immersive-extension
        </a></p>
<p>GRASS GIS module for importing data from Kinect v2 <br>
    <a href="https://github.com/tangible-landscape/r.in.kinect">
        github.com/tangible-landscape/r.in.kinect
    </a></p>
<p>Tangible Landscape repository on Open Science Framework <br>
    <a href="https://osf.io/w8nr6/">
        osf.io/w8nr6
    </a></p>

<img width="20%" src="https://github.com/ncsu-geoforall-lab/tangible-landscape-talk/raw/master/img/logos/gpl.png">
<aside class="notes">

This system and all other development made by our team is free and open source and I are committed to help you setting up your own Tangible landscape system.
</aside>

</section>


<section>
<h3>Resources</h3>

<ul>
    <li>Tangible Landscape website:  <a href="https://tangible-landscape.github.io">tangible-landscape.github.io</a></li>
    <li>Tangible Landscape wiki: <br><a href="https://github.com/tangible-landscape/grass-tangible-landscape/wiki">github.com/tangible-landscape/grass-tangible-landscape/wiki</a> </li>
    <li>Book:
      <ul>

        <li><a href="http://www.springer.com/us/book/9783319893020"><em>Tangible Modeling with Open Source GIS 2nd ed</em></a></li>
        <li><a href="http://www.springer.com/us/book/9783319257730"><em>Tangible Modeling with Open Source GIS 1st ed</em></a></li>
      </ul>
<li><a href="https://www.researchgate.net/publication/309458110_Immersive_Tangible_Geospatial_Modeling">
    Immersive Tangible Geospatial Modeling.</a> Proceedings of ACM SIGSPATIAL 2016.</li>
    <li><a href="https://www.researchgate.net/publication/318846696_Tangible_Immersion_for_Ecological_Design">
    Tangible Immersion for ecological design </a> Proceedings of ACADIA 2017.</li>

</ul>


<img width="20%" src="https://github.com/ncsu-geoforall-lab/tangible-landscape-talk/raw/master/img/tl_book_cover.png">
<img  class="stretch" src="https://github.com/ncsu-geoforall-lab/tangible-landscape-talk/raw/master/img/logos/tl_logo.png">
<aside class="notes">
If you are interested to learn more about Tangible landscape, These are some useful resources that can get you started.

</aside>


</section> -->


<!----SLIDE 28 Video-- -->


























</div>  <!-- slides -->

</div>  <!-- reveal -->

<script src="lib/js/head.min.js"></script>
<script src="js/reveal.js"></script>

<script>

    // Full list of configuration options available here:
    // https://github.com/hakimel/reveal.js#configuration
    Reveal.initialize({
        // Display controls in the bottom right corner
        controls: false,

        // Display a presentation progress bar
        progress: true,

        center: true,

        // Display the page number of the current slide
        slideNumber: false,

        // Enable the slide overview mode
        overview: true,

        // Turns fragments on and off globally
        fragments: true,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
         width: 1060,
        // height: 700,

        // Factor of the display size that should remain empty around the content
        margin: 0.05,  // increase?

        // Bounds for smallest/largest possible scale to apply to content
        minScale: 0.5,
        maxScale: 5.0,

        theme: Reveal.getQueryHash().theme,  // available themes are in /css/theme
        transition: Reveal.getQueryHash().transition || 'none', // default/cube/page/concave/zoom/linear/fade/none

        // Push each slide change to the browser history
        history: true,
        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Vertical centering of slides
        center: true,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,
        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // Hides the address bar on mobile devices
        hideAddressBar: true,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition speed
        transitionSpeed: 'default', // default/fast/slow

        // Transition style for full page slide backgrounds
        backgroundTransition: 'none', // default/none/slide/concave/convex/zoom

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Parallax background image
        //parallaxBackgroundImage: '', // e.g. "'https://s3.amazonaws.com/hakim-static/reveal-js/reveal-parallax-1.jpg'"

        // Parallax background size
        //parallaxBackgroundSize: '' // CSS syntax, e.g. "2100px 900px"
        chalkboard: {
    // optionally load pre-recorded chalkboard drawing from file
        src: "chalkboard.json",
            color: [ 'rgb(255, 38, 0)', 'rgba(255,255,255,0.5)' ]
      },

        math: {
          		mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js',
          		config: 'TeX-AMS_HTML-full'  // See http://docs.mathjax.org/en/latest/config-files.html
      },

        dependencies: [
      	{ src: 'plugin/math/math.js', async: true }
      ],

        // Optional libraries used to extend on reveal.js
        dependencies: [
            { src: 'lib/js/classList.js', condition: function() { return !document.body.classList; } },
            { src: 'plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
            { src: 'plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
            { src: 'plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },
            { src: 'plugin/zoom-js/zoom.js', async: true, condition: function() { return !!document.body.classList; } },
            { src: 'plugin/notes/notes.js', async: true, condition: function() { return !!document.body.classList; } },
            { src: 'plugin/math/math.js', async: true },
            { src: 'plugin/chalkboard/chalkboard.js' }
        ],
        keyboard: {
          67: function() { RevealChalkboard.toggleNotesCanvas() },	// toggle notes canvas when 'c' is pressed
          66: function() { RevealChalkboard.toggleChalkboard() },	// toggle chalkboard when 'b' is pressed
          46: function() { RevealChalkboard.clear() },	// clear chalkboard when 'DEL' is pressed
           8: function() { RevealChalkboard.reset() },	// reset chalkboard data on current slide when 'BACKSPACE' is pressed
          68: function() { RevealChalkboard.download() },	// downlad recorded chalkboard drawing when 'd' is pressed
      },
    });

</script>

</body>
</html>
